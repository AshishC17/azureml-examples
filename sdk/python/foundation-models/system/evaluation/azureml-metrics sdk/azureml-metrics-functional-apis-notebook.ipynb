{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo on Azureml-Metrics Package\n",
    "This notebook provides a comprehensive demonstration of utilizing the azureml-metrics package. The demonstration covers the following key aspects:\n",
    "\n",
    "1. **List Task** : Provides an overview of all tasks supported by the azureml-metrics package.\n",
    "\n",
    "2. **List Metrics** :\n",
    "\n",
    "    a. Lists all metrics for all available tasks.\n",
    "\n",
    "    b. Lists metrics specific to a given task.\n",
    "\n",
    "3. **List Prompt** : Displays the prompt associated with a metric supported by a given task.\n",
    "\n",
    "4. **Score API** : Illustrates how to compute metrics directly using the model and test data.\n",
    "\n",
    "4. **Compute Metrics API** : \n",
    "\n",
    "    a. Demonstrates computing task-specific metrics.\n",
    "    \n",
    "    b. Shows how to compute metrics without specifying the task type.\n",
    "\n",
    "5. **Computing Custom Prompt Metrics**:\n",
    "\n",
    "    a. Guides you through computing custom prompt metrics alongside task-supported metrics.\n",
    "\n",
    "    b. Explains how to compute custom prompt metrics without specifying the task type.\n",
    "\n",
    "\n",
    "This demonstration will help you effectively leverage the capabilities of the azureml-metrics package for a variety of tasks related to metric computation and management.\n",
    "\n",
    "#### Prerequisites\n",
    " \n",
    "Please install the latest version of azureml-metrics package (text based requirements) using the following command:\n",
    " \n",
    "``` $ pip install --upgrade azureml-metrics[all] ```\n",
    " \n",
    "For more details on azureml-metrics package, please refer to the following link: https://aka.ms/azureml-metrics-quick-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat-completion',\n",
       " 'classification',\n",
       " 'code-generation',\n",
       " 'custom-prompt-metric',\n",
       " 'fill-mask',\n",
       " 'forecasting',\n",
       " 'image-classification',\n",
       " 'image-classification-multilabel',\n",
       " 'image-instance-segmentation',\n",
       " 'image-multi-labeling',\n",
       " 'image-object-detection',\n",
       " 'qa',\n",
       " 'qa_multiple_ground_truth',\n",
       " 'rag-evaluation',\n",
       " 'regression',\n",
       " 'summarization',\n",
       " 'text-classification',\n",
       " 'text-classification-multilabel',\n",
       " 'text-generation',\n",
       " 'text-ner',\n",
       " 'translation'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import list_tasks\n",
    "supported_tasks = list_tasks()\n",
    "supported_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) List metrics for all the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b09b1 td {\n",
       "  text-align: justify;\n",
       "}\n",
       "#T_b09b1 th {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b09b1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b09b1_level0_col0\" class=\"col_heading level0 col0\" >TASKS</th>\n",
       "      <th id=\"T_b09b1_level0_col1\" class=\"col_heading level0 col1\" >SUPPORTED METRICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_b09b1_row0_col0\" class=\"data row0 col0\" >classification</td>\n",
       "      <td id=\"T_b09b1_row0_col1\" class=\"data row0 col1\" >{'f1_score_weighted', 'iou_macro', 'precision_score_binary', 'f1_score_macro', 'average_precision_score_weighted', 'iou', 'accuracy_table', 'iou_micro', 'average_precision_score_binary', 'norm_macro_recall', 'f1_score_binary', 'recall_score_macro', 'precision_score_weighted', 'precision_score_macro', 'recall_score_micro', 'confusion_matrix', 'AUC_macro', 'matthews_correlation', 'iou_weighted', 'AUC_classwise', 'f1_score_micro', 'balanced_accuracy', 'average_precision_score_micro', 'weighted_accuracy', 'accuracy', 'AUC_micro', 'average_precision_score_macro', 'recall_score_weighted', 'classification_report', 'precision_score_micro', 'recall_score_classwise', 'recall_score_binary', 'average_precision_score_classwise', 'AUC_weighted', 'precision_score_classwise', 'AUC_binary', 'log_loss', 'iou_classwise', 'f1_score_classwise'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_b09b1_row1_col0\" class=\"data row1 col0\" >regression</td>\n",
       "      <td id=\"T_b09b1_row1_col1\" class=\"data row1 col1\" >{'spearman_correlation', 'predicted_true', 'r2_score', 'mean_absolute_percentage_error', 'explained_variance', 'median_absolute_error', 'root_mean_squared_log_error', 'normalized_root_mean_squared_error', 'mean_absolute_error', 'root_mean_squared_error', 'normalized_median_absolute_error', 'normalized_root_mean_squared_log_error', 'residuals', 'normalized_mean_absolute_error'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_b09b1_row2_col0\" class=\"data row2 col0\" >text-classification</td>\n",
       "      <td id=\"T_b09b1_row2_col1\" class=\"data row2 col1\" >{'f1_score_weighted', 'iou_macro', 'precision_score_binary', 'f1_score_macro', 'average_precision_score_weighted', 'iou', 'accuracy_table', 'iou_micro', 'average_precision_score_binary', 'norm_macro_recall', 'f1_score_binary', 'recall_score_macro', 'precision_score_weighted', 'precision_score_macro', 'recall_score_micro', 'confusion_matrix', 'AUC_macro', 'matthews_correlation', 'iou_weighted', 'AUC_classwise', 'f1_score_micro', 'balanced_accuracy', 'average_precision_score_micro', 'weighted_accuracy', 'accuracy', 'AUC_micro', 'average_precision_score_macro', 'recall_score_weighted', 'classification_report', 'precision_score_micro', 'recall_score_classwise', 'recall_score_binary', 'average_precision_score_classwise', 'AUC_weighted', 'precision_score_classwise', 'AUC_binary', 'log_loss', 'iou_classwise', 'f1_score_classwise'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_b09b1_row3_col0\" class=\"data row3 col0\" >text-classification-multilabel</td>\n",
       "      <td id=\"T_b09b1_row3_col1\" class=\"data row3 col1\" >{'f1_score_weighted', 'iou_macro', 'precision_score_binary', 'f1_score_macro', 'average_precision_score_weighted', 'iou', 'accuracy_table', 'iou_micro', 'average_precision_score_binary', 'norm_macro_recall', 'f1_score_binary', 'recall_score_macro', 'precision_score_weighted', 'precision_score_macro', 'recall_score_micro', 'confusion_matrix', 'AUC_macro', 'matthews_correlation', 'iou_weighted', 'AUC_classwise', 'f1_score_micro', 'balanced_accuracy', 'average_precision_score_micro', 'weighted_accuracy', 'accuracy', 'AUC_micro', 'average_precision_score_macro', 'recall_score_weighted', 'classification_report', 'precision_score_micro', 'recall_score_classwise', 'recall_score_binary', 'average_precision_score_classwise', 'AUC_weighted', 'precision_score_classwise', 'AUC_binary', 'log_loss', 'iou_classwise', 'f1_score_classwise'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_b09b1_row4_col0\" class=\"data row4 col0\" >text-ner</td>\n",
       "      <td id=\"T_b09b1_row4_col1\" class=\"data row4 col1\" >{'f1_score_weighted', 'recall_score_weighted', 'f1_score_macro', 'precision_score_weighted', 'precision_score_micro', 'accuracy', 'recall_score_macro', 'precision_score_macro', 'recall_score_micro', 'f1_score_micro'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_b09b1_row5_col0\" class=\"data row5 col0\" >translation</td>\n",
       "      <td id=\"T_b09b1_row5_col1\" class=\"data row5 col1\" >{'bleu_2', 'bleu_3', 'bleu_4', 'bleu_1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_b09b1_row6_col0\" class=\"data row6 col0\" >summarization</td>\n",
       "      <td id=\"T_b09b1_row6_col1\" class=\"data row6 col1\" >{'rouge1', 'rouge2', 'rougeL', 'rougeLsum'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_b09b1_row7_col0\" class=\"data row7 col0\" >qa</td>\n",
       "      <td id=\"T_b09b1_row7_col1\" class=\"data row7 col1\" >{'llm_coherence', 'bertscore', 'llm_groundedness', 'gpt_groundedness', 'gpt_coherence', 'ada_similarity', 'gpt_similarity', 'gpt_fluency', 'llm_similarity', 'llm_relevance', 'exact_match', 'gpt_relevance', 'llm_fluency', 'f1_score'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_b09b1_row8_col0\" class=\"data row8 col0\" >qa_multiple_ground_truth</td>\n",
       "      <td id=\"T_b09b1_row8_col1\" class=\"data row8 col1\" >{'llm_coherence', 'bertscore', 'llm_groundedness', 'gpt_groundedness', 'gpt_coherence', 'ada_similarity', 'gpt_similarity', 'gpt_fluency', 'llm_similarity', 'llm_relevance', 'exact_match', 'gpt_relevance', 'llm_fluency', 'f1_score'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
       "      <td id=\"T_b09b1_row9_col0\" class=\"data row9 col0\" >fill-mask</td>\n",
       "      <td id=\"T_b09b1_row9_col1\" class=\"data row9 col1\" >{'perplexity'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row10\" class=\"row_heading level0 row10\" >11</th>\n",
       "      <td id=\"T_b09b1_row10_col0\" class=\"data row10 col0\" >text-generation</td>\n",
       "      <td id=\"T_b09b1_row10_col1\" class=\"data row10 col1\" >{'perplexity', 'bleu_4', 'bleu_2', 'rouge1', 'rouge2', 'rougeLsum', 'bleu_1', 'rougeL', 'bleu_3'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row11\" class=\"row_heading level0 row11\" >12</th>\n",
       "      <td id=\"T_b09b1_row11_col0\" class=\"data row11 col0\" >chat-completion</td>\n",
       "      <td id=\"T_b09b1_row11_col1\" class=\"data row11 col1\" >{'perplexity', 'bleu_4', 'gpt_groundedness', 'conversation_groundedness_score', 'rouge1', 'rouge2', 'rougeLsum', 'gpt_retrieval_score', 'gpt_relevance', 'bleu_1', 'rougeL', 'bleu_2', 'bleu_3'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row12\" class=\"row_heading level0 row12\" >13</th>\n",
       "      <td id=\"T_b09b1_row12_col0\" class=\"data row12 col0\" >rag-evaluation</td>\n",
       "      <td id=\"T_b09b1_row12_col1\" class=\"data row12 col1\" >{'gpt_retrieval_score', 'gpt_groundedness', 'gpt_relevance'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row13\" class=\"row_heading level0 row13\" >14</th>\n",
       "      <td id=\"T_b09b1_row13_col0\" class=\"data row13 col0\" >code-generation</td>\n",
       "      <td id=\"T_b09b1_row13_col1\" class=\"data row13 col1\" >{'bleu_4', 'code_eval', 'rouge1', 'rouge2', 'rougeLsum', 'rougeL', 'bleu_1', 'bleu_2', 'bleu_3'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row14\" class=\"row_heading level0 row14\" >15</th>\n",
       "      <td id=\"T_b09b1_row14_col0\" class=\"data row14 col0\" >image-object-detection</td>\n",
       "      <td id=\"T_b09b1_row14_col1\" class=\"data row14 col1\" >['per_label_metrics', 'mean_average_precision', 'image_level_binary_classsifier_metrics', 'confusion_matrices_per_score_threshold', 'recall', 'precision']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row15\" class=\"row_heading level0 row15\" >16</th>\n",
       "      <td id=\"T_b09b1_row15_col0\" class=\"data row15 col0\" >image-instance-segmentation</td>\n",
       "      <td id=\"T_b09b1_row15_col1\" class=\"data row15 col1\" >['precision', 'mean_average_precision', 'per_label_metrics', 'recall']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row16\" class=\"row_heading level0 row16\" >17</th>\n",
       "      <td id=\"T_b09b1_row16_col0\" class=\"data row16 col0\" >forecasting</td>\n",
       "      <td id=\"T_b09b1_row16_col1\" class=\"data row16 col1\" >{'spearman_correlation', 'forecast_time_series_id_distribution_table', 'r2_score', 'mean_absolute_percentage_error', 'explained_variance', 'median_absolute_error', 'root_mean_squared_log_error', 'normalized_root_mean_squared_error', 'mean_absolute_error', 'root_mean_squared_error', 'normalized_median_absolute_error', 'normalized_root_mean_squared_log_error', 'forecast_table', 'forecast_mean_absolute_percentage_error', 'forecast_residuals', 'normalized_mean_absolute_error'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row17\" class=\"row_heading level0 row17\" >18</th>\n",
       "      <td id=\"T_b09b1_row17_col0\" class=\"data row17 col0\" >image-classification</td>\n",
       "      <td id=\"T_b09b1_row17_col1\" class=\"data row17 col1\" >{'f1_score_weighted', 'iou_macro', 'precision_score_binary', 'f1_score_macro', 'average_precision_score_weighted', 'iou', 'accuracy_table', 'iou_micro', 'average_precision_score_binary', 'norm_macro_recall', 'f1_score_binary', 'recall_score_macro', 'precision_score_weighted', 'precision_score_macro', 'recall_score_micro', 'confusion_matrix', 'AUC_macro', 'matthews_correlation', 'iou_weighted', 'AUC_classwise', 'f1_score_micro', 'balanced_accuracy', 'average_precision_score_micro', 'weighted_accuracy', 'accuracy', 'AUC_micro', 'average_precision_score_macro', 'recall_score_weighted', 'classification_report', 'precision_score_micro', 'recall_score_classwise', 'recall_score_binary', 'average_precision_score_classwise', 'AUC_weighted', 'precision_score_classwise', 'AUC_binary', 'log_loss', 'iou_classwise', 'f1_score_classwise'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row18\" class=\"row_heading level0 row18\" >19</th>\n",
       "      <td id=\"T_b09b1_row18_col0\" class=\"data row18 col0\" >image-classification-multilabel</td>\n",
       "      <td id=\"T_b09b1_row18_col1\" class=\"data row18 col1\" >{'f1_score_weighted', 'iou_macro', 'precision_score_binary', 'f1_score_macro', 'average_precision_score_weighted', 'iou', 'accuracy_table', 'iou_micro', 'average_precision_score_binary', 'norm_macro_recall', 'f1_score_binary', 'recall_score_macro', 'precision_score_weighted', 'precision_score_macro', 'recall_score_micro', 'confusion_matrix', 'AUC_macro', 'matthews_correlation', 'iou_weighted', 'AUC_classwise', 'f1_score_micro', 'balanced_accuracy', 'average_precision_score_micro', 'weighted_accuracy', 'accuracy', 'AUC_micro', 'average_precision_score_macro', 'recall_score_weighted', 'classification_report', 'precision_score_micro', 'recall_score_classwise', 'recall_score_binary', 'average_precision_score_classwise', 'AUC_weighted', 'precision_score_classwise', 'AUC_binary', 'log_loss', 'iou_classwise', 'f1_score_classwise'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row19\" class=\"row_heading level0 row19\" >20</th>\n",
       "      <td id=\"T_b09b1_row19_col0\" class=\"data row19 col0\" >image-multi-labeling</td>\n",
       "      <td id=\"T_b09b1_row19_col1\" class=\"data row19 col1\" >{'f1_score_weighted', 'iou_macro', 'precision_score_binary', 'f1_score_macro', 'average_precision_score_weighted', 'iou', 'accuracy_table', 'iou_micro', 'average_precision_score_binary', 'norm_macro_recall', 'f1_score_binary', 'recall_score_macro', 'precision_score_weighted', 'precision_score_macro', 'recall_score_micro', 'confusion_matrix', 'AUC_macro', 'matthews_correlation', 'iou_weighted', 'AUC_classwise', 'f1_score_micro', 'balanced_accuracy', 'average_precision_score_micro', 'weighted_accuracy', 'accuracy', 'AUC_micro', 'average_precision_score_macro', 'recall_score_weighted', 'classification_report', 'precision_score_micro', 'recall_score_classwise', 'recall_score_binary', 'average_precision_score_classwise', 'AUC_weighted', 'precision_score_classwise', 'AUC_binary', 'log_loss', 'iou_classwise', 'f1_score_classwise'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09b1_level0_row20\" class=\"row_heading level0 row20\" >21</th>\n",
       "      <td id=\"T_b09b1_row20_col0\" class=\"data row20 col0\" >video-multi-object-tracking</td>\n",
       "      <td id=\"T_b09b1_row20_col1\" class=\"data row20 col1\" >['per_label_metrics', 'mean_average_precision', 'image_level_binary_classsifier_metrics', 'confusion_matrices_per_score_threshold', 'recall', 'precision', 'FM', 'MOTP', 'FP', 'PT', 'ML', 'MT', 'recall', 'Tracking_Precision', 'Tracking_Recall', 'IDSw', 'IDF1', 'FN', 'MOTA', 'precision']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24032fb9a50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "metrics = list_metrics()\n",
    "\n",
    "# to display the metrics as a dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "flattened_data = [(key, values) for item in metrics for key, values in item.items()]\n",
    "metrics_df = pd.DataFrame(flattened_data, columns=['TASKS','SUPPORTED METRICS']).set_index(pd.RangeIndex(start=1, stop=len(flattened_data)+1))\n",
    "\n",
    "# styling the dataframe\n",
    "metrics_df =  metrics_df.style.set_table_styles([\n",
    "                {'selector': 'td', 'props': [('text-align', 'justify')]},\n",
    "                {'selector': 'th', 'props': [('text-align', 'center')]}\n",
    "            ])\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) List metrics for a specific Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada_similarity',\n",
       " 'bertscore',\n",
       " 'exact_match',\n",
       " 'f1_score',\n",
       " 'gpt_coherence',\n",
       " 'gpt_fluency',\n",
       " 'gpt_groundedness',\n",
       " 'gpt_relevance',\n",
       " 'gpt_similarity',\n",
       " 'llm_coherence',\n",
       " 'llm_fluency',\n",
       " 'llm_groundedness',\n",
       " 'llm_relevance',\n",
       " 'llm_similarity'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "metrics = list_metrics('qa')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Prompt\n",
    "Lists the prompt of a metric, supported by the given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. Consider the overall quality of the answer when evaluating coherence. Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:\n",
      "One star: the answer completely lacks coherence\n",
      "Two stars: the answer mostly lacks coherence\n",
      "Three stars: the answer is partially coherent\n",
      "Four stars: the answer is mostly coherent\n",
      "Five stars: the answer has perfect coherency\n",
      "\n",
      "This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n",
      "\n",
      "question: What is your favorite indoor activity and why do you enjoy it?\n",
      "answer: I like pizza. The sun is shining.\n",
      "stars: 1\n",
      "\n",
      "question: Can you describe your favorite movie without giving away any spoilers?\n",
      "answer: It is a science fiction movie. There are dinosaurs. The actors eat cake. People must stop the villain.\n",
      "stars: 2\n",
      "\n",
      "question: What are some benefits of regular exercise?\n",
      "answer: Regular exercise improves your mood. A good workout also helps you sleep better. Trees are green.\n",
      "stars: 3\n",
      "\n",
      "question: How do you cope with stress in your daily life?\n",
      "answer: I usually go for a walk to clear my head. Listening to music helps me relax as well. Stress is a part of life, but we can manage it through some activities.\n",
      "stars: 4\n",
      "\n",
      "question: What can you tell me about climate change and its effects on the environment?\n",
      "answer: Climate change has far-reaching effects on the environment. Rising temperatures result in the melting of polar ice caps, contributing to sea-level rise. Additionally, more frequent and severe weather events, such as hurricanes and heatwaves, can cause disruption to ecosystems and human societies alike.\n",
      "stars: 5\n",
      "\n",
      "question: \n",
      "answer: \n",
      "stars:\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import list_prompts\n",
    "\n",
    "coherence_prompt = list_prompts(task_type='qa', metric='gpt_coherence')\n",
    "print(coherence_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score API\n",
    "Computing metrics directly with a model and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing classification metrics: 100%|██████████████████████████████████████████████| 28/28 [00:00<00:00, 1374.65it/s]\n",
      "Metrics skipped due to missing y_pred_proba:\n",
      " ['average_precision_score_micro', 'average_precision_score_weighted', 'accuracy_table', 'average_precision_score_binary', 'AUC_micro', 'norm_macro_recall', 'average_precision_score_macro', 'AUC_macro', 'AUC_weighted', 'AUC_binary', 'log_loss']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'confusion_matrix': {'data': {'class_labels': ['0', '1', '2'],\n",
      "                                             'matrix': [[50, 0, 0],\n",
      "                                                        [0, 47, 3],\n",
      "                                                        [0, 1, 49]]},\n",
      "                                    'schema_type': 'confusion_matrix',\n",
      "                                    'schema_version': '1.0.0'}},\n",
      " 'metrics': {'accuracy': 0.9733333333333334,\n",
      "             'balanced_accuracy': 0.9733333333333333,\n",
      "             'f1_score_binary': nan,\n",
      "             'f1_score_macro': 0.9733226623982927,\n",
      "             'f1_score_micro': 0.9733333333333334,\n",
      "             'f1_score_weighted': 0.9733226623982927,\n",
      "             'matthews_correlation': 0.9602561024455323,\n",
      "             'precision_score_binary': nan,\n",
      "             'precision_score_macro': 0.9738247863247862,\n",
      "             'precision_score_micro': 0.9733333333333334,\n",
      "             'precision_score_weighted': 0.9738247863247864,\n",
      "             'recall_score_binary': nan,\n",
      "             'recall_score_macro': 0.9733333333333333,\n",
      "             'recall_score_micro': 0.9733333333333334,\n",
      "             'recall_score_weighted': 0.9733333333333334,\n",
      "             'weighted_accuracy': 0.9733333333333333}}\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import score\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "metrics = score(task_type='classification',\n",
    "                model=clf,\n",
    "                X_test=X,\n",
    "                y_test=y)\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_params = {\n",
    "    \"api_version\": '<placeholder>',\n",
    "    \"api_base\": '<placeholder>',\n",
    "    \"api_type\": '<placeholder>',\n",
    "    \"api_key\": '<placeholder>',\n",
    "    \"deployment_id\": '<placeholder>'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing QA dataset\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "\n",
    "y_test = [coherent_answer, coherent_answer]\n",
    "y_pred = [coherent_answer, incoherent_answer]\n",
    "contexts = [context, context]\n",
    "questions = [question, question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Computing Task supported Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM related metrics need llm_params to be computed. Computing metrics for ['bertscore', 'gpt_groundedness', 'gpt_coherence', 'ada_similarity', 'gpt_similarity', 'gpt_fluency', 'exact_match', 'gpt_relevance', 'f1_score']\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.36it/s]\n",
      "Using the engine text-embedding-ada-002 for computing ada similarity. Please ensure to have valid deployment for text-embedding-ada-002 model\n",
      "Could not compute metric because of the following exception : RetryError[<Future at 0x2404c1fa7a0 state=finished raised InvalidRequestError>]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'mean_bertscore_precision': 0.6587736904621124,\n",
       "  'mean_bertscore_recall': 0.6381914019584656,\n",
       "  'mean_bertscore_f1': 0.6499990373849869,\n",
       "  'mean_gpt_groundedness': 4.0,\n",
       "  'median_gpt_groundedness': 4.0,\n",
       "  'mean_gpt_coherence': 4.0,\n",
       "  'median_gpt_coherence': 4.0,\n",
       "  'mean_ada_similarity': nan,\n",
       "  'median_ada_similarity': nan,\n",
       "  'mean_gpt_similarity': 3.0,\n",
       "  'median_gpt_similarity': 3.0,\n",
       "  'mean_gpt_fluency': 4.0,\n",
       "  'median_gpt_fluency': 4.0,\n",
       "  'mean_exact_match': 0.5,\n",
       "  'median_exact_match': 0.5,\n",
       "  'mean_gpt_relevance': 3.0,\n",
       "  'median_gpt_relevance': 3.0,\n",
       "  'mean_f1_score': 0.625,\n",
       "  'median_f1_score': 0.625},\n",
       " 'artifacts': {'bertscore': {'precision': [1.0, 0.31754738092422485],\n",
       "   'recall': [1.0, 0.27638280391693115],\n",
       "   'f1': [1.0, 0.29999807476997375],\n",
       "   'hashcode': 'microsoft/deberta-large_L16_no-idf_version=0.3.12(hug_trans=4.28.1)-rescaled'},\n",
       "  'gpt_groundedness': [5, 3],\n",
       "  'gpt_coherence': [5, 3],\n",
       "  'ada_similarity': ['retryerror', 'retryerror'],\n",
       "  'gpt_similarity': [5, 1],\n",
       "  'gpt_fluency': [5, 3],\n",
       "  'exact_match': [True, False],\n",
       "  'gpt_relevance': [5, 1],\n",
       "  'f1_score': [1.0, 0.25]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics\n",
    "\n",
    "metrics = compute_metrics(task_type='qa', y_test=y_test, y_pred=y_pred, questions=questions, contexts=contexts, openai_params=openai_params)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)Computing Metrics without the Task Type\n",
    "Here we compute a subset of metrics without passing the task_type. In this code accuracy for example is not a supported metric for QA task, but we can still pass and calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We have unused keyword arguments : ['questions', 'contexts', 'openai_params']\n",
      "Applicable keyword arguments for text-ner are ['metrics'].\n",
      "Assertion Failed. Invalid Operation. Target: y_test_value. Reference Code: validate_ner. Details: y_test_value must be a list\n",
      "Assertion Failed. Invalid Operation. Target: y_test_value. Reference Code: validate_ner. Details: y_test_value must be a list\n",
      "Skipping the computation of ['accuracy'] for text-ner task due to the following exception : Assertion Failed. Invalid Operation. Target: y_test_value. Reference Code: validate_ner. Details: y_test_value must be a list\n",
      "LLM related metrics need llm_params to be computed. Computing metrics for ['f1_score', 'gpt_relevance']\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.43it/s]\n",
      "Assertion Failed. Invalid Operation. Target: y_test_value. Reference Code: validate_chat_completion. Details: y_test_value must be a list\n",
      "Assertion Failed. Invalid Operation. Target: y_test_value. Reference Code: validate_chat_completion. Details: y_test_value must be a list\n",
      "Skipping the computation of ['gpt_relevance'] for chat-completion task due to the following exception : Assertion Failed. Invalid Operation. Target: y_test_value. Reference Code: validate_chat_completion. Details: y_test_value must be a list\n",
      "Assertion Failed. Invalid Operation. Target: y_pred. Reference Code: validate_rag_evaluation. Details: every conversation must be a list or dictionary. please check conversation_number: 1\n",
      "Assertion Failed. Invalid Operation. Target: y_pred. Reference Code: validate_rag_evaluation. Details: every conversation must be a list or dictionary. please check conversation_number: 1\n",
      "Skipping the computation of ['gpt_relevance'] for rag-evaluation task due to the following exception : Assertion Failed. Invalid Operation. Target: y_pred. Reference Code: validate_rag_evaluation. Details: every conversation must be a list or dictionary. please check conversation_number: 1\n",
      "Computing classification metrics: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Computing classification metrics: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Computing classification metrics: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'mean_f1_score': 0.625,\n",
       "  'median_f1_score': 0.625,\n",
       "  'mean_gpt_relevance': 3.0,\n",
       "  'median_gpt_relevance': 3.0,\n",
       "  'accuracy': 0.5},\n",
       " 'artifacts': {'f1_score': [1.0, 0.25], 'gpt_relevance': [5, 1]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics\n",
    "\n",
    "metrics_config = {\n",
    "    \"metrics\": ['accuracy','f1_score', 'gpt_relevance']\n",
    "}\n",
    "\n",
    "metrics = compute_metrics(y_test=y_test, y_pred=y_pred, questions=questions, contexts=contexts, openai_params=openai_params, **metrics_config)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Custom Prompt Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom prompt templates for the metrics to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom coherence prompt : \n",
      " Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. Consider the overall quality of the answer when evaluating coherence. Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:\n",
      "One star: the answer completely lacks coherence\n",
      "Two stars: the answer mostly lacks coherence\n",
      "Three stars: the answer is partially coherent\n",
      "Four stars: the answer is mostly coherent\n",
      "Five stars: the answer has perfect coherency\n",
      "\n",
      "This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n",
      "\n",
      "question: What is your favorite indoor activity and why do you enjoy it?\n",
      "answer: I like pizza. The sun is shining.\n",
      "stars: 1\n",
      "\n",
      "question: Can you describe your favorite movie without giving away any spoilers?\n",
      "answer: It is a science fiction movie. There are dinosaurs. The actors eat cake. People must stop the villain.\n",
      "stars: 2\n",
      "\n",
      "question: What are some benefits of regular exercise?\n",
      "answer: Regular exercise improves your mood. A good workout also helps you sleep better. Trees are green.\n",
      "stars: 3\n",
      "\n",
      "question: How do you cope with stress in your daily life?\n",
      "answer: I usually go for a walk to clear my head. Listening to music helps me relax as well. Stress is a part of life, but we can manage it through some activities.\n",
      "stars: 4\n",
      "\n",
      "question: What can you tell me about climate change and its effects on the environment?\n",
      "answer: Climate change has far-reaching effects on the environment. Rising temperatures result in the melting of polar ice caps, contributing to sea-level rise. Additionally, more frequent and severe weather events, such as hurricanes and heatwaves, can cause disruption to ecosystems and human societies alike.\n",
      "stars: 5\n",
      "\n",
      "question: {{questions}}\n",
      "answer: {{predictions}}\n",
      "stars:\n"
     ]
    }
   ],
   "source": [
    "custom_coherence_prompt = 'Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. Consider the overall quality of the answer when evaluating coherence. Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:\\nOne star: the answer completely lacks coherence\\nTwo stars: the answer mostly lacks coherence\\nThree stars: the answer is partially coherent\\nFour stars: the answer is mostly coherent\\nFive stars: the answer has perfect coherency\\n\\nThis rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\\n\\nquestion: What is your favorite indoor activity and why do you enjoy it?\\nanswer: I like pizza. The sun is shining.\\nstars: 1\\n\\nquestion: Can you describe your favorite movie without giving away any spoilers?\\nanswer: It is a science fiction movie. There are dinosaurs. The actors eat cake. People must stop the villain.\\nstars: 2\\n\\nquestion: What are some benefits of regular exercise?\\nanswer: Regular exercise improves your mood. A good workout also helps you sleep better. Trees are green.\\nstars: 3\\n\\nquestion: How do you cope with stress in your daily life?\\nanswer: I usually go for a walk to clear my head. Listening to music helps me relax as well. Stress is a part of life, but we can manage it through some activities.\\nstars: 4\\n\\nquestion: What can you tell me about climate change and its effects on the environment?\\nanswer: Climate change has far-reaching effects on the environment. Rising temperatures result in the melting of polar ice caps, contributing to sea-level rise. Additionally, more frequent and severe weather events, such as hurricanes and heatwaves, can cause disruption to ecosystems and human societies alike.\\nstars: 5\\n\\nquestion: {{questions}}\\nanswer: {{predictions}}\\nstars:'\n",
    "print(\"Custom coherence prompt : \\n\", custom_coherence_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom equivalence metric3 : \n",
      " Equivalence, as a metric, measures the similarity between the predicted answer and the correct answer. If the information and content in the predicted answer is similar or equivalent to the correct answer, then the value of the Equivalence metric should be high, else it should be low. Given the question, correct answer, and predicted answer, determine the value of Equivalence metric using the following rating scale: One star: the predicted answer is not at all similar to the correct answer Two stars: the predicted answer is mostly not similar to the correct answer Three stars: the predicted answer is somewhat similar to the correct answer Four stars: the predicted answer is mostly similar to the correct answer Five stars: the predicted answer is completely similar to the correct answer This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5. The examples below show the Equivalence score for a question, a correct answer, and a predicted answer. question: What is the role of ribosomes? correct answer: Ribosomes are cellular structures responsible for protein synthesis. They interpret the genetic information carried by messenger RNA (mRNA) and use it to assemble amino acids into proteins. predicted answer: Ribosomes participate in carbohydrate breakdown by removing nutrients from complex sugar molecules. stars: 1 question: Why did the Titanic sink? correct answer: The Titanic sank after it struck an iceberg during its maiden voyage in 1912. The impact caused the ship's hull to breach, allowing water to flood into the vessel. The ship's design, lifeboat shortage, and lack of timely rescue efforts contributed to the tragic loss of life. predicted answer: The sinking of the Titanic was a result of a large iceberg collision. This caused the ship to take on water and eventually sink, leading to the death of many passengers due to a shortage of lifeboats and insufficient rescue attempts. stars: 2 question: What causes seasons on Earth? correct answer: Seasons on Earth are caused by the tilt of the Earth's axis and its revolution around the Sun. As the Earth orbits the Sun, the tilt causes different parts of the planet to receive varying amounts of sunlight, resulting in changes in temperature and weather patterns. predicted answer: Seasons occur because of the Earth's rotation and its elliptical orbit around the Sun. The tilt of the Earth's axis causes regions to be subjected to different sunlight intensities, which leads to temperature fluctuations and alternating weather conditions. stars: 3 question: How does photosynthesis work? correct answer: Photosynthesis is a process by which green plants and some other organisms convert light energy into chemical energy. This occurs as light is absorbed by chlorophyll molecules, and then carbon dioxide and water are converted into glucose and oxygen through a series of reactions. predicted answer: In photosynthesis, sunlight is transformed into nutrients by plants and certain microorganisms. Light is captured by chlorophyll molecules, followed by the conversion of carbon dioxide and water into sugar and oxygen through multiple reactions. stars: 4 question: What are the health benefits of regular exercise? correct answer: Regular exercise can help maintain a healthy weight, increase muscle and bone strength, and reduce the risk of chronic diseases. It also promotes mental well-being by reducing stress and improving overall mood. predicted answer: Routine physical activity can contribute to  maintaining ideal body weight, enhancing muscle and bone strength, and preventing chronic illnesses. In addition, it supports mental health by alleviating stress and augmenting general mood. \n",
      "stars: 5\n",
      "\n",
      " question: {{questions}}\n",
      " correct answer: {{ground_truths}}\n",
      " predicted answer: {{predictions}}\n",
      " stars:\n"
     ]
    }
   ],
   "source": [
    "custom_equivalence_metric = \"Equivalence, as a metric, measures the similarity between the predicted answer and the correct answer. If the information and content in the predicted answer is similar or equivalent to the correct answer, then the value of the Equivalence metric should be high, else it should be low. Given the question, correct answer, and predicted answer, determine the value of Equivalence metric using the following rating scale: One star: the predicted answer is not at all similar to the correct answer Two stars: the predicted answer is mostly not similar to the correct answer Three stars: the predicted answer is somewhat similar to the correct answer Four stars: the predicted answer is mostly similar to the correct answer Five stars: the predicted answer is completely similar to the correct answer This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5. The examples below show the Equivalence score for a question, a correct answer, and a predicted answer. question: What is the role of ribosomes? correct answer: Ribosomes are cellular structures responsible for protein synthesis. They interpret the genetic information carried by messenger RNA (mRNA) and use it to assemble amino acids into proteins. predicted answer: Ribosomes participate in carbohydrate breakdown by removing nutrients from complex sugar molecules. stars: 1 question: Why did the Titanic sink? correct answer: The Titanic sank after it struck an iceberg during its maiden voyage in 1912. The impact caused the ship's hull to breach, allowing water to flood into the vessel. The ship's design, lifeboat shortage, and lack of timely rescue efforts contributed to the tragic loss of life. predicted answer: The sinking of the Titanic was a result of a large iceberg collision. This caused the ship to take on water and eventually sink, leading to the death of many passengers due to a shortage of lifeboats and insufficient rescue attempts. stars: 2 question: What causes seasons on Earth? correct answer: Seasons on Earth are caused by the tilt of the Earth's axis and its revolution around the Sun. As the Earth orbits the Sun, the tilt causes different parts of the planet to receive varying amounts of sunlight, resulting in changes in temperature and weather patterns. predicted answer: Seasons occur because of the Earth's rotation and its elliptical orbit around the Sun. The tilt of the Earth's axis causes regions to be subjected to different sunlight intensities, which leads to temperature fluctuations and alternating weather conditions. stars: 3 question: How does photosynthesis work? correct answer: Photosynthesis is a process by which green plants and some other organisms convert light energy into chemical energy. This occurs as light is absorbed by chlorophyll molecules, and then carbon dioxide and water are converted into glucose and oxygen through a series of reactions. predicted answer: In photosynthesis, sunlight is transformed into nutrients by plants and certain microorganisms. Light is captured by chlorophyll molecules, followed by the conversion of carbon dioxide and water into sugar and oxygen through multiple reactions. stars: 4 question: What are the health benefits of regular exercise? correct answer: Regular exercise can help maintain a healthy weight, increase muscle and bone strength, and reduce the risk of chronic diseases. It also promotes mental well-being by reducing stress and improving overall mood. predicted answer: Routine physical activity can contribute to  maintaining ideal body weight, enhancing muscle and bone strength, and preventing chronic illnesses. In addition, it supports mental health by alleviating stress and augmenting general mood. \\nstars: 5\\n\\n question: {{questions}}\\n correct answer: {{ground_truths}}\\n predicted answer: {{predictions}}\\n stars:\"\"\"\n",
    "\n",
    "print(\"Custom equivalence metric3 : \\n\", custom_equivalence_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are preparing the prompts using `AzuremlCustomPromptMetric` API, and creating the extra metrics list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import AzureMLCustomPromptMetric\n",
    "\n",
    "custom_metric_names = [\"custom_coherence_class_api\", \"custom_equivalence_class_api\"]\n",
    "custom_metric_descriptions = [\"Custom coherence metric with class based implementation\",\n",
    "                              \"Custom equivalence metric with class based implementation\"]\n",
    "\n",
    "user_prompt_templates = [custom_coherence_prompt, custom_equivalence_metric]\n",
    "input_vars = [[\"questions\", \"predictions\"], [\"questions\", \"ground_truths\", \"predictions\"]]\n",
    "\n",
    "custom_coherence_prompt_config = {\n",
    "    \"input_vars\" : input_vars[0],\n",
    "    \"openai_params\" : openai_params,\n",
    "    \"metric_name\" : custom_metric_names[0],\n",
    "    \"metric_description\" : custom_metric_descriptions[0],\n",
    "    \"user_prompt_template\" : user_prompt_templates[0],\n",
    "}\n",
    "\n",
    "custom_equivalence_prompt_config = {\n",
    "    \"input_vars\" : input_vars[1],\n",
    "    \"openai_params\" : openai_params,\n",
    "    \"metric_name\" : custom_metric_names[1],\n",
    "    \"metric_description\" : custom_metric_descriptions[1],\n",
    "    \"user_prompt_template\" : user_prompt_templates[1],\n",
    "}\n",
    "\n",
    "custom_coherence_class_based = AzureMLCustomPromptMetric(**custom_coherence_prompt_config)\n",
    "custom_equivalence_class_based = AzureMLCustomPromptMetric(**custom_equivalence_prompt_config)\n",
    "\n",
    "extra_metrics = [custom_coherence_class_based, custom_equivalence_class_based]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Computing custom prompt metrics along with task supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "We have unused keyword arguments : ['predictions', 'ground_truths']\n",
      "Applicable keyword arguments for qa are ['metrics', 'tokenizer', 'regexes_to_ignore', 'ignore_case', 'ignore_punctuation', 'ignore_numbers', 'lang', 'model_type', 'questions', 'openai_params', 'idf', 'rescale_with_baseline', 'contexts', 'openai_api_batch_size', 'use_chat_completion_api', 'openai_embedding_engine', 'llm_params', 'llm_api_batch_size'].\n",
      "LLM related metrics need llm_params to be computed. Computing metrics for ['bertscore', 'gpt_groundedness', 'gpt_coherence', 'ada_similarity', 'gpt_similarity', 'gpt_fluency', 'exact_match', 'gpt_relevance', 'f1_score']\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.38it/s]\n",
      "Using the engine text-embedding-ada-002 for computing ada similarity. Please ensure to have valid deployment for text-embedding-ada-002 model\n",
      "Could not compute metric because of the following exception : RetryError[<Future at 0x2404c265210 state=finished raised InvalidRequestError>]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'mean_custom_coherence_class_api': 4.0,\n",
       "  'median_custom_coherence_class_api': 4.0,\n",
       "  'mean_custom_equivalence_class_api': 3.0,\n",
       "  'median_custom_equivalence_class_api': 3.0,\n",
       "  'mean_bertscore_precision': 0.6587736904621124,\n",
       "  'mean_bertscore_recall': 0.6381914019584656,\n",
       "  'mean_bertscore_f1': 0.6499990373849869,\n",
       "  'mean_gpt_groundedness': 4.0,\n",
       "  'median_gpt_groundedness': 4.0,\n",
       "  'mean_gpt_coherence': 4.0,\n",
       "  'median_gpt_coherence': 4.0,\n",
       "  'mean_ada_similarity': nan,\n",
       "  'median_ada_similarity': nan,\n",
       "  'mean_gpt_similarity': 3.0,\n",
       "  'median_gpt_similarity': 3.0,\n",
       "  'mean_gpt_fluency': 4.0,\n",
       "  'median_gpt_fluency': 4.0,\n",
       "  'mean_exact_match': 0.5,\n",
       "  'median_exact_match': 0.5,\n",
       "  'mean_gpt_relevance': 3.0,\n",
       "  'median_gpt_relevance': 3.0,\n",
       "  'mean_f1_score': 0.625,\n",
       "  'median_f1_score': 0.625},\n",
       " 'artifacts': {'custom_coherence_class_api': ['5', '3'],\n",
       "  'custom_equivalence_class_api': ['5', '1'],\n",
       "  'bertscore': {'precision': [1.0, 0.31754738092422485],\n",
       "   'recall': [1.0, 0.27638280391693115],\n",
       "   'f1': [1.0, 0.29999807476997375],\n",
       "   'hashcode': 'microsoft/deberta-large_L16_no-idf_version=0.3.12(hug_trans=4.28.1)-rescaled'},\n",
       "  'gpt_groundedness': [5, 3],\n",
       "  'gpt_coherence': [5, 3],\n",
       "  'ada_similarity': ['retryerror', 'retryerror'],\n",
       "  'gpt_similarity': [5, 1],\n",
       "  'gpt_fluency': [5, 3],\n",
       "  'exact_match': [True, False],\n",
       "  'gpt_relevance': [5, 1],\n",
       "  'f1_score': [1.0, 0.25]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics\n",
    "\n",
    "data_input = {\n",
    "    # additional data required for question answering metrics\n",
    "    \"contexts\": contexts,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"y_test\": y_test,\n",
    "\n",
    "    # data required for coherence, equivalence prompt template\n",
    "    \"questions\": questions,\n",
    "    \"predictions\": y_pred,\n",
    "    \"ground_truths\": y_test,\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type='qa',\n",
    "                        metrics=extra_metrics,\n",
    "                        openai_params=openai_params,\n",
    "                        **data_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Computing custom prompt metrics without a task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'mean_custom_coherence_class_api': 4.0,\n",
       "  'median_custom_coherence_class_api': 4.0,\n",
       "  'mean_custom_equivalence_class_api': 3.0,\n",
       "  'median_custom_equivalence_class_api': 3.0},\n",
       " 'artifacts': {'custom_coherence_class_api': ['5', '3'],\n",
       "  'custom_equivalence_class_api': ['5', '1']}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics\n",
    "\n",
    "custom_prompt_data_input = {\n",
    "    # data required for coherence prompt template\n",
    "    \"questions\" : questions,\n",
    "    \"predictions\": y_pred,\n",
    "    # additional data required for equivalence prompt template\n",
    "    \"ground_truths\": y_test,\n",
    "}\n",
    "\n",
    "result = compute_metrics(metrics=extra_metrics,\n",
    "                        **custom_prompt_data_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
