{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat group examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatGroup, ChatAgent\n",
    "\n",
    "copilot_agent = ChatAgent(name=\"copilot_flow\", flow=\"./promotflow_copilot\")\n",
    "simulation_agent = ChatAgent(name=\"simulation_flow\", flow=\"./promptflow_simulation\")\n",
    "# for eager flow, may use below\n",
    "# simulation_agent = ChatAgent(name=\"simulation_flow\", eager_flow=entry_func)\n",
    "\n",
    "with ChatGroup(\n",
    "    max_turns=10,\n",
    "    max_token=5000,\n",
    "    max_time=600,  # 10 minutes\n",
    "    entry_agent=copilot_agent,  # the first agent to speak\n",
    "    speak_order=[copilot_agent, simulation_agent],  # other than specifying, can also be \"LLM\" or \"AUTO\"(default)\n",
    ") as chat_group:\n",
    "    copilot_agent.io_mapping(\n",
    "        question=simulation_agent.outputs.generated_question, \n",
    "        model=\"gpt4\",\n",
    "    )\n",
    "    simulation_agent.io_mapping(\n",
    "        persona=\"Tom\", \n",
    "        last_answer=copilot_agent.outputs.output,\n",
    "        chat_history=chat_group.chat_history,  # chat_history is a group-level context\n",
    "        model=\"gpt4\",\n",
    "        goal=data_column,  # not sure the original purpose\n",
    "    )\n",
    "    # simulation_agent.terminate_func = lambda x: x == \"<END>\"\n",
    "\n",
    "    # kick off the chat\n",
    "    chat_group.run()\n",
    "\n",
    "    # access to the agent outputs\n",
    "    print(\"Chat history:\", chat_group.chat_history)\n",
    "    print(\"Last copilot flow output:\", copilot_agent.outputs[\"output\"])\n",
    "    print(\"Last simulation flow output:\", simulation_agent.outputs[\"generated_question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatGroup, ChatAgent\n",
    "\n",
    "copilot_agent = ChatAgent(name=\"copilot_flow\", flow=\"./promotflow_copilot\")\n",
    "simulation_agent = ChatAgent(name=\"simulation_flow\", flow=\"./promptflow_simulation\")\n",
    "\n",
    "chat_group = ChatGroup(\n",
    "    agents=[copilot_agent, simulation_agent],\n",
    "    max_turns=10,\n",
    "    max_token=5000,\n",
    "    max_time=600,  # 10 minutes\n",
    "    entry_agent=copilot_agent,  # the first agent to speak\n",
    "    speak_order=[copilot_agent, simulation_agent],  # other than specifying, can also be \"LLM\" or \"AUTO\"(default)\n",
    "    io_mapping = {\n",
    "        \"copilot_flow.question\": \"${simulation_flow.outputs.generated_question}\",\n",
    "        \"copilot_flow.model\": \"gpt4\",  # model is an external parameter\n",
    "        \"simulation_flow.persona\": \"Tom\", \n",
    "        \"simulation_flow.chat_history\": \"${group.chat_history}\",  # chat_history is a group-level context\n",
    "        \"simulation_flow.model\": \"gpt4\",\n",
    "        \"simulation_flow.goal\": \"<jsonl_data.column1>\",  # not sure the original purpose\n",
    "    }\n",
    ")\n",
    "\n",
    "# kick off the chat\n",
    "chat_group.run()\n",
    "\n",
    "# access to the agent outputs\n",
    "print(\"Chat history:\", chat_group.chat_history)\n",
    "print(\"Last copilot flow output:\", copilot_agent.outputs[\"output\"])\n",
    "print(\"Last simulation flow output:\", simulation_agent.outputs[\"generated_question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatGroup, ChatAgent\n",
    "\n",
    "copilot_agent = ChatAgent(name=\"copilot_flow\", flow=\"./promotflow_copilot\")\n",
    "simulation_agent = ChatAgent(name=\"simulation_flow\", flow=\"./promptflow_simulation\")\n",
    "# for eager flow, may use below\n",
    "# simulation_agent = ChatAgent(name=\"simulation_flow\", eager_flow=entry_func)\n",
    "\n",
    "chat_group  = ChatGroup(\n",
    "    agents=[copilot_agent, simulation_agent],\n",
    "    max_turns=10,\n",
    "    max_token=5000,\n",
    "    max_time=600,  # 10 minutes\n",
    "    entry_agent=copilot_agent,  # the first agent to speak\n",
    "    inputs={\n",
    "        \"question\": {\n",
    "            \"type\": str,\n",
    "            \"default\": \"What is GPT-4?\",\n",
    "        }\n",
    "    }\n",
    "    outputs={\n",
    "        \"output\": {\n",
    "            \"type\": str,\n",
    "            \"reference\": copilot_agent.outputs.output,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "copilot_agent.set_inputs(\n",
    "    question=chat_group.inputs.question,\n",
    "    model=\"gpt4\",\n",
    ")\n",
    "\n",
    "simulation_agent.set_inputs(\n",
    "    persona=\"Tom\", \n",
    "    last_answer=copilot_agent.outputs.output,\n",
    "    chat_history=chat_group.chat_history,  # chat_history is a group-level context\n",
    "    model=\"gpt4\",\n",
    ")\n",
    "\n",
    "# kick off the chat\n",
    "chat_group.invoke(questioin=\"What is GPT-4?\")\n",
    "\n",
    "# access to the agent outputs\n",
    "print(\"Chat history:\", chat_group.chat_history)\n",
    "print(\"Last copilot flow output:\", copilot_agent.outputs.output.value)\n",
    "print(\"Last simulation flow output:\", simulation_agent.outputs.generated_question.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK Option 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import chat_group, ChatAgent\n",
    "\n",
    "copilot_agent = ChatAgent(name=\"copilot_flow\", flow=\"./promotflow_copilot\")\n",
    "simulation_agent = ChatAgent(name=\"simulation_flow\", flow=\"./promptflow_simulation\")\n",
    "# for eager flow, may use below\n",
    "# simulation_agent = ChatAgent(name=\"simulation_flow\", eager_flow=entry_func)\n",
    "\n",
    "@chat_group(\n",
    "    max_turns=10,\n",
    "    max_token=5000,\n",
    "    max_time=600,  # 10 minutes\n",
    "    speak_order=\"definition\"\n",
    ")\n",
    "def my_chat_group_flow(question: str) -> str:\n",
    "    # Note:\n",
    "    #   1. Chat history is no longer group level context, need agent to manage it\n",
    "    #   2. Need to set entry agent like \"copilot.is_entry = True\"\n",
    "    #   3. Cannot use \"speak_order\" to specify the order of the agents, definition order is the order\n",
    "    #   4. Cannot specify partial agents to run, or set \"copilot_agent.mute = True\"\n",
    "    copilot_agent.set_inputs(\n",
    "        question=chat_group.inputs.question,  \n",
    "        model=\"gpt4\",\n",
    "    )\n",
    "    copilot_agent.is_entry = True\n",
    "\n",
    "    simulation_agent.set_inputs(\n",
    "        persona=\"Tom\", \n",
    "        last_answer=copilot_agent.outputs.output,\n",
    "        chat_history=chat_group.chat_history,  # how to refer to the chat history \n",
    "        model=\"gpt4\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"output\": copilot_agent.outputs.output,\n",
    "    }\n",
    "\n",
    "# kick off the chat\n",
    "my_chat_group_flow(question=\"What is GPT-4?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original yaml in experiment\n",
    "```yaml\n",
    "# multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 5\n",
    "    agents:\n",
    "      - name: copilot_flow\n",
    "        flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "        inputs:\n",
    "          question: ${roles.simulation_flow.outputs.generated_question}\n",
    "          chat_history: []\n",
    "          model: ${inputs.model_name}\n",
    "      - name: simulation_flow\n",
    "        flow: ../evaluation/similarity.yaml\n",
    "        inputs:\n",
    "          persona: ${inputs.persona}\n",
    "          model: ${inputs.model_name}\n",
    "          goal: ${data.leo_min_set.query}\n",
    "          chat_history: ${group.chat_history}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml option 1 - Exp\n",
    "```yaml\n",
    "# multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 10\n",
    "    max_token: 5000\n",
    "    max_time: 600  # 10 minutes\n",
    "    entry_agent: copilot_flow\n",
    "    speak_order: [copilot_flow, simulation_flow]\n",
    "    agents:\n",
    "      - name: copilot_flow\n",
    "        flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "        inputs:\n",
    "          question: ${roles.simulation_flow.outputs.generated_question}\n",
    "          chat_history: []\n",
    "          model: ${inputs.model_name}\n",
    "      - name: simulation_flow\n",
    "        flow: ../evaluation/similarity.yaml\n",
    "        inputs:\n",
    "          persona: ${inputs.persona}\n",
    "          model: ${inputs.model_name}\n",
    "          chat_history: ${group.chat_history}\n",
    "        terminate_signal: \"<END>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml option 1 - Flow\n",
    "```yaml\n",
    "name: multi_turn_chat\n",
    "type: chat_group\n",
    "max_turns: 10\n",
    "max_token: 5000\n",
    "max_time: 600  # 10 minutes\n",
    "inputs:\n",
    "  question:\n",
    "    type: str\n",
    "  persona:\n",
    "    type: str\n",
    "  model_name:\n",
    "    type: str\n",
    "outputs:\n",
    "  # chat history output may become built-in\n",
    "  chat_history:\n",
    "    type: list\n",
    "  answer:\n",
    "    type: str\n",
    "    reference: ${copilot_flow.outputs.output}\n",
    "speak_order: [copilot_flow, simulation_flow]\n",
    "agents:\n",
    "  - name: copilot_flow\n",
    "    flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "    inputs:\n",
    "      question: ${agents.simulation_flow.outputs.generated_question}\n",
    "      chat_history: []\n",
    "      model: ${inputs.model_name}\n",
    "  - name: simulation_flow\n",
    "    flow: ../evaluation/similarity.yaml\n",
    "    inputs:\n",
    "      persona: ${inputs.persona}\n",
    "      model: ${inputs.model_name}\n",
    "      chat_history: ${group.chat_history}\n",
    "    terminate_signal: \"<END>\"  # default stop signal\n",
    "    # terminate_function: \"<function source json>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "agents:\n",
    "  ...\n",
    "  - name: simulation_flow\n",
    "    flow: ../evaluation/similarity.yaml\n",
    "    inputs:\n",
    "      persona: ${inputs.persona}\n",
    "      model: ${inputs.model_name}\n",
    "      chat_history: ${group.chat_history}\n",
    "    terminate_signal: \"<END>\"  # default stop signal\n",
    "    # terminate_function: \"<function source json>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml option - Inputs/Outputs over conversation history\n",
    "```yaml\n",
    "name: multi_turn_chat\n",
    "type: chat_group\n",
    "max_turns: 10\n",
    "inputs:\n",
    "  topic:\n",
    "    type: str\n",
    "    default: movie\n",
    "    description: \"The topic of the conversation\"\n",
    "  model_name:\n",
    "    type: str\n",
    "    default: gpt-3.5-turbo\n",
    "    description: \"The model name to use for the conversation\"\n",
    "outputs:\n",
    "  # chat history output may become built-in output\n",
    "  chat_history:\n",
    "    type: list\n",
    "  result:\n",
    "    type: str\n",
    "    reference: ${assistant.outputs.output}\n",
    "roles:\n",
    "  - role: assistant\n",
    "    flow: ./copilot_flow\n",
    "    inputs:\n",
    "      question: ${user.outputs.generated_question}\n",
    "      chat_history: []\n",
    "      model: ${inputs.model_name}\n",
    "    initializer:\n",
    "      question: ${inputs.topic}\n",
    "  - role: user\n",
    "    flow: ./simulation_flow\n",
    "    inputs:\n",
    "      model: ${inputs.model_name}\n",
    "      chat_history: ${chat_group.chat_history}\n",
    "    terminate_signal: \"<END>\"  # default stop signal\n",
    "    # terminate_function: \"<function source json>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml option 2\n",
    "\n",
    "```yaml\n",
    "# multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 10\n",
    "    max_token: 5000\n",
    "    max_time: 600  # 10 minutes\n",
    "    agents:\n",
    "      - name: copilot_flow\n",
    "        flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "      - name: simulation_flow\n",
    "        flow: ../evaluation/similarity.yaml\n",
    "    entry_agent: copilot_flow\n",
    "    speak_order: [copilot_flow, simulation_flow]\n",
    "    io_mapping:\n",
    "        copilot_flow.question: ${simulation_flow.outputs.generated_question}\n",
    "        copilot_flow.model: ${inputs.model_name}\n",
    "        copilot_flow.chat_history: []  # maybe omit?\n",
    "        simulation_flow.persona: ${inputs.persona}\n",
    "        simulation_flow.chat_history: ${group.chat_history}  # chat_history is a group-level context\n",
    "        simulation_flow.model: ${inputs.model_name}\n",
    "        simulation_flow.goal: ${data.leo_min_set.query}  # not sure the original purpose\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml Option 3 - Conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "inputs:\n",
    "  - name: model_name\n",
    "    type: string\n",
    "    default: gpt-4\n",
    "  - name: persona\n",
    "    type: string\n",
    "    default: default\n",
    "\n",
    "data: \n",
    "  - name: leo_min_set\n",
    "    type: jsonl\n",
    "    path: leo_query_set.jsonl\n",
    "\n",
    "nodes:\n",
    "  # multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 5\n",
    "    stop_signal: \"[STOP]\"\n",
    "    roles:\n",
    "      - #name: copilot_flow\n",
    "        role: assistant\n",
    "        path: flow.dag.yaml\n",
    "        inputs:\n",
    "          topic: ${data.leo_min_set.query}\n",
    "          model: ${inputs.model_name}\n",
    "          conversation_history: ${parent.conversation_history}\n",
    "      - # name: simulation_flow\n",
    "        role: user\n",
    "        path: azureml://registries/azureml/models/similarity/versions/5\n",
    "        inputs:\n",
    "          topic: ${data.leo_min_set.query}\n",
    "          conversation_history: ${parent.conversation_history}\n",
    "\n",
    "  # evaluate for Leo metrics\n",
    "  - name: leo_eval\n",
    "    type: flow\n",
    "    path: azureml://registries/azureml/models/leo_eval/versions/5\n",
    "    inputs:\n",
    "      model: ${inputs.model_name}\n",
    "      metrics: swiss_leo, ground_leo, pi_leo\n",
    "      query: ${data.leo_min_set.query}\n",
    "      answer: ${nodes.multi_turn_chat.conversation_history}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatAgent\n",
    "\n",
    "def terminate_chat(result):\n",
    "    # result is the flow result that should be a dict\n",
    "    return result[\"score\"] >= 10\n",
    "\n",
    "simulation_agent = ChatAgent(\n",
    "    name=\"simulation_flow\", \n",
    "    flow=\"./promptflow_simulation\",\n",
    "    terminate_func=terminate_chat,  # set customized terminate function\n",
    ")\n",
    "\n",
    "# this is the default terminate function if not specified\n",
    "def default_terminate_function(result):\n",
    "    # this would require the flow result has only one output, not a dict\n",
    "    return result[\"output\"] == \"<END>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "[\n",
    "    [\"copilot_agent\", \"chat message 1\"],\n",
    "    [\"simulation_agent\", \"chat message 2\"],\n",
    "    [\"copilot_agent\", \"chat message 3\"],\n",
    "]\n",
    "\n",
    "# Option 2\n",
    "[\n",
    "    \"<copilot_agent>: chat message 1\",\n",
    "    \"<simulation_agent>: chat message 2\",\n",
    "    \"<copilot_agent>: chat message 3\",\n",
    "]\n",
    "\n",
    "# or maybe save both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from promptflow._sdk.entities._chat_group._chat_group import ChatGroup\n",
    "from promptflow._sdk.entities._chat_group._chat_role import ChatRole\n",
    "\n",
    "TEST_ROOT = Path(r\"E:\\programs\\msft-promptflow\\src\\promptflow\\tests\")\n",
    "FLOWS_DIR = TEST_ROOT / \"test_configs/flows\"\n",
    "\n",
    "question = \"What's the most beautiful thing in the world?\"\n",
    "ground_truth = \"The world itself.\"\n",
    "\n",
    "copilot = ChatRole(\n",
    "    flow=FLOWS_DIR / \"chat_group_copilot\",\n",
    "    role=\"assistant\",\n",
    "    inputs=dict(\n",
    "        question=question,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        conversation_history=\"${parent.conversation_history}\",\n",
    "    ),\n",
    ")\n",
    "simulation = ChatRole(\n",
    "    flow=FLOWS_DIR / \"chat_group_simulation\",\n",
    "    role=\"user\",\n",
    "    inputs=dict(\n",
    "        question=question,\n",
    "        ground_truth=ground_truth,\n",
    "        conversation_history=\"${parent.conversation_history}\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat_group = ChatGroup(\n",
    "    roles=[copilot, simulation],\n",
    "    max_turns=4,\n",
    "    max_tokens=1000,\n",
    "    max_time=1000,\n",
    "    stop_signal=\"[STOP]\",\n",
    ")\n",
    "chat_group.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = dict()\n",
    "\n",
    "print(list(a.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGroupOrchestratorProxy:\n",
    "    def __init__(self, xxx):\n",
    "        from promptflow._sdk.entities import ChatGroup, ChatRole\n",
    "\n",
    "        # initialize the chat group object\n",
    "        self._chat_group_obj = ChatGroup(\n",
    "            roles=[\n",
    "                copilot = ChatRole(flow=role_path_1, role=\"assistant\"),\n",
    "                simulation = ChatRole(flow=role_path_2, role=\"user\"),\n",
    "            ],\n",
    "            max_turns=4,\n",
    "            stop_signal=\"[STOP]\",\n",
    "        )\n",
    "\n",
    "    async def exec_line_async(\n",
    "        self,\n",
    "        inputs: Mapping[str, Any],\n",
    "        index: Optional[int] = None,\n",
    "        run_id: Optional[str] = None,\n",
    "    ) -> LineResult:\n",
    "        \"\"\"Run a single line data through the chat group object.\"\"\"\n",
    "        # copy object for concurrent execution\n",
    "        executed_chat_group_obj = copy.deepcopy(self._chat_group_obj)\n",
    "        # update inputs\n",
    "        executed_chat_group_obj._update_role_inputs(inputs)\n",
    "        # invoke the chat group object\n",
    "        await executed_chat_group_obj.async_invoke()\n",
    "        return process_output(executed_chat_group_obj.conversation_history, index, run_id)\n",
    "\n",
    "# run the batch engine\n",
    "batch_engine = BatchEngine(xxx)\n",
    "chat_group_orchestrator_proxy = ChatGroupOrchestratorProxy(xxx)\n",
    "result = batch_engine.run(xx, executor_proxy=chat_group_orchestrator_proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "def merge_jsonl_files(source_folder: str, output_folder: str, group_size: int = 25) -> None:\n",
    "    source_folder_path = Path(source_folder)\n",
    "    output_folder_path = Path(output_folder)\n",
    "\n",
    "    jsonl_files = sorted(source_folder_path.glob('*.jsonl'))\n",
    "\n",
    "    for i in range(0, len(jsonl_files), group_size):\n",
    "        group = jsonl_files[i:i+group_size]\n",
    "        output_file_name = f\"{group[0].stem.split('_')[0]}_{group[-1].stem.split('_')[0]}.jsonl\"\n",
    "        output_file_path = output_folder_path / output_file_name\n",
    "\n",
    "        with output_file_path.open('w') as output_file:\n",
    "            for jsonl_file in group:\n",
    "                with jsonl_file.open() as input_file:\n",
    "                    json_line = json.load(input_file)\n",
    "                    json.dump(json_line, output_file, ensure_ascii=False)\n",
    "                    output_file.write('\\n')\n",
    "\n",
    "source_folder = r\"C:\\Users\\hod\\.promptflow\\.runs\\simple_hello_world_variant_0_20240330_152442_732549\\flow_artifacts\"\n",
    "output_folder = r\"C:\\Users\\hod\\.promptflow\\.runs\\simple_hello_world_variant_0_20240330_152442_732549\\flow_artifacts_merged\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "merge_jsonl_files(source_folder, output_folder, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-09T15:27:46.470184\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "a = datetime.now()\n",
    "print(a.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
