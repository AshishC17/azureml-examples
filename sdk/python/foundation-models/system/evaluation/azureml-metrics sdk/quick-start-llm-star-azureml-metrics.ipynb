{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bcbb15",
   "metadata": {},
   "source": [
    "#### Computing Prompt based (LLM as a Judge/LLM Star) metrics in AzureML SDK\n",
    "\n",
    "This sample notebook demonstrates how to compute LLM as a Judge based metrics in AzureML SDK. The notebook demonstrates the following steps:\n",
    "\n",
    "<pre>\n",
    "1) Calculate any prompt based metrics using openAI GPT models\n",
    "2) Calculate any prompt based metrics using any deployed LLM endpoint from AzureML Model Catalog\n",
    "</pre>\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "1) Please install the latest version of azureml-metrics package (text based requirements) using the following command:\n",
    "\n",
    "``` $ pip install --upgrade azureml-metrics[text] ```\n",
    "\n",
    "For more details on azureml-metrics package, please refer to the following link: https://aka.ms/azureml-metrics-quick-start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26160e5e",
   "metadata": {},
   "source": [
    "#### 1) Calculate any prompt based metrics using openAI GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398ee0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM related metrics need llm_params to be computed. Computing metrics for ['gpt_relevance', 'gpt_groundedness', 'gpt_coherence', 'gpt_similarity', 'gpt_fluency']\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'mean_gpt_relevance': 3.0,\n",
       "  'median_gpt_relevance': 3.0,\n",
       "  'mean_gpt_groundedness': 4.0,\n",
       "  'median_gpt_groundedness': 4.0,\n",
       "  'mean_gpt_coherence': 4.0,\n",
       "  'median_gpt_coherence': 4.0,\n",
       "  'mean_gpt_similarity': 3.0,\n",
       "  'median_gpt_similarity': 3.0,\n",
       "  'mean_gpt_fluency': 4.0,\n",
       "  'median_gpt_fluency': 4.0},\n",
       " 'artifacts': {'gpt_relevance': [5, 1],\n",
       "  'gpt_groundedness': [5, 3],\n",
       "  'gpt_coherence': [5, 3],\n",
       "  'gpt_similarity': [5, 1],\n",
       "  'gpt_fluency': [5, 3]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# Note: Please replace the values for the following variables with your own values.\n",
    "openai_params = {\n",
    "    \"api_version\" : os.environ[\"OPENAI_API_VERSION\"],\n",
    "    \"api_base\" : os.environ[\"OPENAI_API_BASE\"],\n",
    "    \"api_type\" : os.environ[\"OPENAI_API_TYPE\"],\n",
    "    \"api_key\" : os.environ[\"OPENAI_API_KEY\"],\n",
    "    \"deployment_id\" : \"<deployment_name>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "    \"questions\" : [question, question],\n",
    "    \"contexts\" : [context, context],\n",
    "    \"openai_params\" : openai_params,\n",
    "    \"metrics\" : [\"gpt_coherence\", \"gpt_fluency\", \"gpt_relevance\", \"gpt_groundedness\", \"gpt_similarity\"]\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING,\n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config,)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139f000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. Consider the overall quality of the answer when evaluating coherence. Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:\n",
      "One star: the answer completely lacks coherence\n",
      "Two stars: the answer mostly lacks coherence\n",
      "Three stars: the answer is partially coherent\n",
      "Four stars: the answer is mostly coherent\n",
      "Five stars: the answer has perfect coherency\n",
      "\n",
      "This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n",
      "\n",
      "question: What is your favorite indoor activity and why do you enjoy it?\n",
      "answer: I like pizza. The sun is shining.\n",
      "stars: 1\n",
      "\n",
      "question: Can you describe your favorite movie without giving away any spoilers?\n",
      "answer: It is a science fiction movie. There are dinosaurs. The actors eat cake. People must stop the villain.\n",
      "stars: 2\n",
      "\n",
      "question: What are some benefits of regular exercise?\n",
      "answer: Regular exercise improves your mood. A good workout also helps you sleep better. Trees are green.\n",
      "stars: 3\n",
      "\n",
      "question: How do you cope with stress in your daily life?\n",
      "answer: I usually go for a walk to clear my head. Listening to music helps me relax as well. Stress is a part of life, but we can manage it through some activities.\n",
      "stars: 4\n",
      "\n",
      "question: What can you tell me about climate change and its effects on the environment?\n",
      "answer: Climate change has far-reaching effects on the environment. Rising temperatures result in the melting of polar ice caps, contributing to sea-level rise. Additionally, more frequent and severe weather events, such as hurricanes and heatwaves, can cause disruption to ecosystems and human societies alike.\n",
      "stars: 5\n",
      "\n",
      "question: \n",
      "answer: \n",
      "stars:\n"
     ]
    }
   ],
   "source": [
    "# for looking at the prompt template for coherence\n",
    "from azureml.metrics import list_prompts, constants\n",
    "\n",
    "coherence_prompt = list_prompts(task_type=constants.Tasks.QUESTION_ANSWERING,\n",
    "                               metric=\"gpt_coherence\")\n",
    "print(coherence_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef2aec",
   "metadata": {},
   "source": [
    "#### 2) Calculate any prompt based metrics using any deployed LLM endpoint from AzureML Model Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e551b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT related metrics need openai_params to be computed. Computing metrics for ['llm_relevance', 'llm_similarity', 'llm_coherence', 'llm_fluency', 'llm_groundedness']\n",
      "GPT related metrics need openai_params in a dictionary.\n",
      "GPT related metrics need openai_params in a dictionary.\n",
      "GPT related metrics need openai_params in a dictionary.\n",
      "GPT related metrics need openai_params in a dictionary.\n",
      "GPT related metrics need openai_params in a dictionary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'mean_llm_relevance': 3.0,\n",
       "  'median_llm_relevance': 3.0,\n",
       "  'mean_llm_similarity': 3.0,\n",
       "  'median_llm_similarity': 3.0,\n",
       "  'mean_llm_coherence': 3.0,\n",
       "  'median_llm_coherence': 3.0,\n",
       "  'mean_llm_fluency': 1.0,\n",
       "  'median_llm_fluency': 1.0,\n",
       "  'mean_llm_groundedness': 1.0,\n",
       "  'median_llm_groundedness': 1.0},\n",
       " 'artifacts': {'llm_relevance': [5, 1],\n",
       "  'llm_similarity': [5, 1],\n",
       "  'llm_coherence': [5, 1],\n",
       "  'llm_fluency': [1, 1],\n",
       "  'llm_groundedness': [1, 1]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# Note: Please replace the values for the following variables with your own values.\n",
    "llm_params = {\n",
    "    \"llm_url\": \"<rest_endpoint_after_deployment_from_azureml_model_catalog>\",\n",
    "    \"llm_api_key\": \"<api_key_for_endpoint>\",\n",
    "    \"azureml_model_deployment\": \"<deployment_name>\",\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "    \"questions\" : [question, question],\n",
    "    \"contexts\" : [context, context],\n",
    "    \"llm_params\" : llm_params,\n",
    "    \"metrics\": [\"llm_coherence\", \"llm_fluency\", \"llm_relevance\", \"llm_groundedness\", \"llm_similarity\"]\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING,\n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config,)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93161b6a",
   "metadata": {},
   "source": [
    "#### How LLM based metrics can be utilized?\n",
    "\n",
    "<pre>\n",
    "1) Use any GPT based models by sending Azure openAI credentials to AzureML Model Evaluation.\n",
    "2) Use any other LLMs in AzureML Model Catalog by deploying them to a real time endpoint.\n",
    "3) Pick any base model: \n",
    "    - perform supervised finetuning with instructions based on computing relevant prompt based metrics.\n",
    "    - onboard into AzureML platform using Import framework.\n",
    "    - deploy the finetuned model to real time endpoint.\n",
    "    - compute prompt based (LLM as a judge/LLM star) metrics using AzureML Model Evaluation.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09647a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
