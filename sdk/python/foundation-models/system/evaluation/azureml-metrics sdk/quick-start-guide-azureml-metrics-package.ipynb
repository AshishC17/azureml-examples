{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3186a0e3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### <font color=\"green\"><center>AzureML Metrics</center></font>\n",
    "<hr>\n",
    "\n",
    "#### <center>AzureML Metrics for Supported Tasks</center>\n",
    "\n",
    "##### Tabular tasks\n",
    "\n",
    "| Task     | Input Format | Supported Metrics | Supported Charts |\n",
    "| :-----------: | :-----------: | :---------------: | :---: |\n",
    "| <a href=\"#single_label_classification\">Single</a> / <a href=\"#multi_label_classification\">Multi Label Classification</a> | Lists, Numpy arrays, DataFrames with single column | log_loss, average_precision_score_binary, weighted_accuracy, AUC_weighted, f1_score_micro, f1_score_binary, precision_score_micro, precision_score_binary, recall_score_weighted, f1_score_weighted, confusion_matrix, average_precision_score_micro, recall_score_binary, recall_score_macro, average_precision_score_weighted, AUC_binary, matthews_correlation, precision_score_macro, accuracy, average_precision_score_macro, AUC_macro, recall_score_micro, balanced_accuracy, f1_score_macro, precision_score_weighted, accuracy_table, AUC_micro, norm_macro_recall | Precision-Recall curve, ROC curve, Calibration curve, Lift curve, Cumulative Gains curve, Confusion Matrix |\n",
    "| <a href=\"#forecasting\">Forecasting</a> | DataFrames, Numpy arrays, string parameters | Same as Regression | forecast_mean_absolute_percentage_error, forecast_residuals, forecast_table, forecast_time_series_id_distribution_table |\n",
    "| <a href=\"#regression\">Regression</a> | Lists, Numpy arrays, DataFrames with Single column | explained_variance, mean_absolute_error, mean_absolute_percentage_error, median_absolute_error, normalized_mean_absolute_error, normalized_median_absolute_error, normalized_root_mean_squared_error, normalized_root_mean_squared_log_error, predicted_true, r2_score, residuals, root_mean_squared_error, root_mean_squared_log_error, spearman_correlation | -- |\n",
    "\n",
    "\n",
    "##### Text based tasks\n",
    "\n",
    "| Task     | Input Format | Supported Metrics | Supported Charts |\n",
    "| :-----------: | :-----------: | :---------------: | :---: |\n",
    "| <a href=\"#single_label_classification\">Single</a> / <a href=\"#multi_label_classification\">Multi Label Classification</a> | Lists, Numpy arrays, DataFrames with single column | log_loss, average_precision_score_binary, weighted_accuracy, AUC_weighted, f1_score_micro, f1_score_binary, precision_score_micro, precision_score_binary, recall_score_weighted, f1_score_weighted, confusion_matrix, average_precision_score_micro, recall_score_binary, recall_score_macro, average_precision_score_weighted, AUC_binary, matthews_correlation, precision_score_macro, accuracy, average_precision_score_macro, AUC_macro, recall_score_micro, balanced_accuracy, f1_score_macro, precision_score_weighted, accuracy_table, AUC_micro, norm_macro_recall | Precision-Recall curve, ROC curve, Calibration curve, Lift curve, Cumulative Gains curve, Confusion Matrix |\n",
    "| <a href=\"#chat-completion\">Chat Completion</a> | List of lists which have string values(y_test), Conversation format (y_pred) | Bleu, Rouge, Perplexity, RAG Metrics (Generation Score, Groudning Score, Retrieval Score) | -- |\n",
    "| <a href=\"#code-generation\">Code Generation</a> | List of lists which have string values, List of strings as test cases | Bleu, Rouge, Pass@k | -- |\n",
    "| <a href=\"#fill-mask\">Fill Mask</a>   | List of strings (only predictions are needed) | Perplexity | -- |\n",
    "| <a href=\"#named-entity-recognition\">Named Entity Recognition</a> | List of Lists which have tokens in string format | precision_score_macro, accuracy, recall_score_micro, f1_score_micro, precision_score_micro, recall_score_weighted, f1_score_macro, precision_score_weighted, f1_score_weighted, recall_score_macro | -- |\n",
    "| <a href=\"#qa\">Question Answering</a>   | List of strings | Exact Match, F1 score, BERT Score, GPT Star metrics (Coherence, Fluency, Groundedness, Relevance, Similarity), Ada Similarity | -- |\n",
    "| <a href=\"#rag-evaluation\">RAG Evaluation</a> | Conversation format (y_pred) | RAG Metrics (Generation Score, Groudning Score, Retrieval Score) | -- |\n",
    "| <a href=\"#summarization\">Summarization</a> | List of Lists which have values in string format | Rouge Metrics(Rouge-{1,2,L,Lsum}) | -- | \n",
    "| <a href=\"#text-generation\">Text generation</a>   | List of Lists which have values in string format | Bleu Metrics(Bleu-{1,2,3,4}), Rouge Metrics(Rouge-{1,2,L,Lsum}), Perplexity | -- |\n",
    "| <a href=\"#translation\">Translation</a>   | List of Lists which have values in string format | Bleu Metrics(Bleu-{1,2,3,4}) | -- |\n",
    "\n",
    "##### Image based tasks\n",
    "\n",
    "\n",
    "| Task     | Input Format | Supported Metrics | Supported Charts |\n",
    "| :-----------: | :-----------: | :---------------: | :---: |\n",
    "| <a href=\"#image-object-detction\">Image Object Detection & Instance Segmentation</a>  | List of Dictionary | Mean Average Precision, Recall, Precision | -- |\n",
    "| <a href=\"#video-related-metrics\">Video Multi-Object Tracking</a>  | List of Dictionary for Groundtruth,  List of numpy array for predictions| MOTA, MOTP, IDF1, IDSw, FM, FN, FP, ML, MT, PT, Tracking_Precision, Tracking_Recall| -- |\n",
    "\n",
    "\n",
    "**Note: All metrics except Perplexity, GPT-{Coherence,Fluency, Groundedness, Relevance}, RAG-{Generation, Grounding, Retrieval} require ground-truth for metrics computation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf9fae",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center>Installing azureml-metrics</center></font>\n",
    "\n",
    "Here are the steps we can follow to install the latest azureml-metrics package:\n",
    "\n",
    "1) Create a new virtual environment\n",
    "\n",
    "```$ conda create --name metrics_env python=3.8```\n",
    "\n",
    "2) Activate the virutal environment \n",
    "\n",
    "```$ conda activate metrics_env```\n",
    "\n",
    "3) Install the latest version of azureml-metrics \n",
    "\n",
    "```$ pip install azureml-metrics[<extra_name>]```\n",
    "\n",
    "**Note: For installing all dependencies please run:**\n",
    "\n",
    "```$ pip install azureml-metrics[all]```\n",
    "\n",
    "4) Install ipykernel, jupyter notebook packages to run jupyter notebook\n",
    "\n",
    "```$ pip install jupyter notebook```\n",
    "\n",
    "```$ pip install ipykernel```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910378f",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center>Support for conditional installation</center></font>\n",
    "\n",
    "**Note: In case if you want to compute only subset of metrics, please run the required command to install only subset of dependencies.**\n",
    "\n",
    "| Metrics/Task type                              | Modality                  | Dependencies                              | Installation Command                   |\n",
    "| ---------------------------------------------- | ------------------------- | ----------------------------------------- | ------------------------------------- |\n",
    "| Any task type                                  | Any modality              | numpy, pandas                              | `$ pip install azureml-metrics`       |\n",
    "| GPT-Star Metrics(QA)                          | Text                      | openai, tenacity, evaluate                  | `$ pip install azureml-metrics[generative-ai]` |\n",
    "| Exact Match(QA)                               | Text                      | Evaluate                                  | `$ pip install azureml-metrics[evaluate]` |\n",
    "| BERTScore(QA)                                 | Text                      | Evaluate, BERTScore, torch, transformers  | `$ pip install azureml-metrics[bert-score]` |\n",
    "| Perplexity(Fill Mask/Text generation/Chat Completion) | Text              | Evaluate, torch, transformers              | `$ pip install azureml-metrics[text]`    |\n",
    "| Bleu (Translation/Text generation)             | Text                      | Evaluate                                  | `$ pip install azureml-metrics[evaluate]` |\n",
    "| Rouge (Summarization/Text generation)          | Text                      | Evaluate, rouge-score                      | `$ pip install azureml-metrics[text]`    |\n",
    "| Rag Evaluation                                | Text                      | Rag-evaluation                             | `$ pip install azureml-metrics[rag-evaluation]`          |\n",
    "| Classification                                |  Tabular/Text              | Scikit-learn                              | `$ pip install azureml-metrics[tabular]` |\n",
    "| Regression/Forecasting Metrics                | Tabular                   | Scikit-learn, scipy                       | `$ pip install azureml-metrics[tabular]` |\n",
    "| Vision metrics                                | Vision                    | Pycocotools                               | `$ pip install azureml-metrics[vision]`  |\n",
    "| Video metrics                                | Video                    | mmcv-full, mmdet, opencv-python-headless, torchvision, mmtrack                               | `$ pip install azureml-metrics[video]`  |\n",
    "| All task types                                | --                        | All of the above                           | `$ pip install azureml-metrics[all]`     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c72dba",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center>List of supported tasks</center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_tasks\n",
    "\n",
    "supported_tasks = list_tasks()\n",
    "print(\", \".join(supported_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cfb91e",
   "metadata": {},
   "source": [
    "## Tabular Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28fe09",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"single_label_classification\">Single Label Classification</a></center></font>\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'class_labels', 'train_labels', 'sample_weight', 'y_transformer', 'use_binary', 'enable_metric_confidence', 'mulitlabel', 'positive_label', 'confidence_metrics']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| class_labels | List for superset of all existing labels in our dataset |\n",
    "| train_labels | List for labels on which model is trained |\n",
    "| sample_weights | List containing the weight assosiated with each data sample |\n",
    "| y_transformer | Transformer object to be applied on y_pred |\n",
    "| use_binary | Compute metrics only on the true class for binary classification |\n",
    "| enable_metric_confidence | Computes confidence interval for supported metrics |\n",
    "| multilabel | Boolean variable that computes multilabel metrics when set to True |\n",
    "| positive_label | Label to be treated as positive label |\n",
    "| confidence_metrics | List of metrics to compute confidence intervals |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d689c",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.CLASSIFICATION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196649b9",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aeac0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=[1,0,1,2,0,2], y_pred=[1,0,1,2,0,2])\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa79e6",
   "metadata": {},
   "source": [
    "#### Testing with y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d63dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=np.array([1,0,1,0]), y_pred=np.array([1,0,1,1]), \n",
    "                         y_pred_proba=np.array([[0.2,0.8], [0.8, 0.2], [0.2, 0.8], [0.4, 0.6]]))\n",
    "pprint(result[\"metrics\"])\n",
    "pprint(result[\"artifacts\"][\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c7ae8",
   "metadata": {},
   "source": [
    "#### Getting hold of unused keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ec4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=np.array([1,0,1,0]), y_pred=np.array([1,0,1,1]), \n",
    "                         y_pred_proba=np.array([[0.2,0.8], [0.8, 0.2], [0.2, 0.8], [0.4, 0.6]]), confidence_metric=True)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98d9a1",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f2053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "metrics_config = {\n",
    "     \"sample_weight\" : [1, 2, 3, 4, 5, 6],\n",
    "     \"enable_metric_confidence\" : True,\n",
    "     \"class_labels\" :[0, 1, 2, 3]\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, \n",
    "                         y_test=[1,0,1,2,0,2], y_pred=[1,0,1,1,0,2],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244382dd",
   "metadata": {},
   "source": [
    "#### Computing Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ae89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import score, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "metrics = score(task_type=constants.Tasks.CLASSIFICATION,\n",
    "                model=clf,\n",
    "                X_test=X,\n",
    "                y_test=y)\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5af05b",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"multi_label_classification\">Multi Label Classification</a></center></font>\n",
    "\n",
    "Here, we can set multilabel argument to True\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'class_labels', 'train_labels', 'sample_weight', 'y_transformer', 'enable_metric_confidence', 'mulitlabel', 'confidence_metrics']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| class_labels | List for superset of all existing labels in our dataset |\n",
    "| train_labels | List for labels on which model is trained |\n",
    "| sample_weights | List containing the weight assosiated with each data sample |\n",
    "| y_transformer | Transformer object to be applied on y_pred |\n",
    "| enable_metric_confidence | Computes confidence interval for supported metrics |\n",
    "| **multilabel** | Boolean variable that computes multilabel metrics when set to True |\n",
    "| confidence_metrics | List of metrics to compute confidence intervals |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51d82d",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.CLASSIFICATION, multilabel=True)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a965b47",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc7916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_test = np.array([[\"politics\", \"sports\"], [\"education\"], [\"movies\", \"politics\"], [\"sports\", \"education\"]], dtype=\"object\")\n",
    "y_pred = np.array([[\"politics\", \"movies\"], [\"sports\", \"education\"], [\"movies\", \"politics\"], [\"sports\", \"politics\"]], dtype=\"object\")\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=y_test, y_pred=y_pred, multilabel=True)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdc15b",
   "metadata": {},
   "source": [
    "#### Testing with y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b94c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "y_test = np.array([[1, 0, 0], [1, 1, 0]])\n",
    "y_pred = np.array([[1, 1, 0], [1, 1, 0]])\n",
    "y_pred_proba = np.array([[0.6, 0.6, 0.2], [0.8, 0.9, 0.1]])\n",
    "\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=y_test, y_pred=y_pred, \n",
    "                         y_pred_proba=y_pred_proba, multilabel=True)\n",
    "pprint(result[\"metrics\"])\n",
    "pprint(result[\"artifacts\"][\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228de6a",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6ba1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "y_test = np.array([[1, 0, 0], [1, 1, 0]])\n",
    "y_pred = np.array([[1, 1, 0], [1, 1, 0]])\n",
    "y_pred_proba = np.array([[0.6, 0.6, 0.2], [0.8, 0.9, 0.1]])\n",
    "\n",
    "metrics_config = {\n",
    "    \"multilabel\" : True, \n",
    "    # equal to the number of data samples\n",
    "    \"sample_weight\" : [200, 100]\n",
    "}\n",
    "\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=y_test, y_pred=y_pred, \n",
    "                         y_pred_proba=y_pred_proba, **metrics_config)\n",
    "pprint(result[\"metrics\"])\n",
    "pprint(result[\"artifacts\"][\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b742e79",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"forecasting\">Forecasting</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'time_column_name', 'X_test', 'X_train', 'y_train', 'y_std', 'sample_weight', 'aggregation_method', 'y_min_dict', 'y_max_dict']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| time_column_name | <font color='red'>Required</font> The time column name. |\n",
    "| X_test | <font color='red'>Required</font> Regressors for the test set. |\n",
    "| metrics | List for subset of metrics to be computed. |\n",
    "| time_series_id_column_names | The time series id column names used to differentiate separate time series in one data set. |\n",
    "| X_train | Regressors for the training set. |\n",
    "| y_train | Target values for the training set. |\n",
    "| y_std | Standard deviation of the target values. |\n",
    "| sample_weight | Unused, added for compatibility. |\n",
    "| aggregation_method | The method used to aggregate metrics over the multiple time series. Default - `numpy.mean` |\n",
    "| y_min_dict | The dictionary, containing historical mimimal values for each time series in the data set. It is an alternative to providing `X_train` and `y_train` |\n",
    "| y_max_dict | The dictionary, containing historical maximal values for each time series in the data set. It is an alternative to providing `X_train` and `y_train` |\n",
    "\n",
    "#### List metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.FORECASTING)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11c044",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ca7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# First we will generate some data.\n",
    "time_column_name = \"date\"\n",
    "X_test = pd.DataFrame({\n",
    "    time_column_name: pd.date_range('2001-01-01', freq='MS', periods=12),\n",
    "})\n",
    "\n",
    "y_actuals = np.arange(12)\n",
    "np.random.seed(42)\n",
    "y_pred = y_actuals + np.random.rand(12)\n",
    "\n",
    "# Compute metrics with the extra parameter num_classes\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.FORECASTING,\n",
    "    y_test=y_actuals,\n",
    "    y_pred=y_pred,\n",
    "    X_test=X_test,\n",
    "    time_column_name=time_column_name,\n",
    "    num_classes=3,\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ae4d5",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d98629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "\n",
    "# We will generate data set, containing two time series.\n",
    "time_column_name = \"date\"\n",
    "time_series_id_column_names = ['ts_id']\n",
    "\n",
    "X_train = pd.DataFrame({\n",
    "    time_column_name: list(pd.date_range(end='2000-12-31', freq='MS', periods=24)) * 2,\n",
    "    'ts_id': np.repeat(['a', 'b'], 24)\n",
    "})\n",
    "y_train = np.concatenate([np.arange(24), np.arange(24)])\n",
    "\n",
    "X_test = pd.DataFrame({\n",
    "    time_column_name: list(pd.date_range('2001-01-01', freq='MS', periods=12)) * 2,\n",
    "    'ts_id': np.repeat(['a', 'b'], 12)\n",
    "})\n",
    "\n",
    "y_actuals = np.concatenate([np.arange(12), np.arange(12)])\n",
    "np.random.seed(42)\n",
    "y_pred = y_actuals + np.random.rand(y_actuals.shape[0])\n",
    "\n",
    "# Compute metrics.\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.FORECASTING,\n",
    "    y_test=y_actuals,\n",
    "    y_pred=y_pred,\n",
    "    X_test=X_test,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    time_column_name=time_column_name,\n",
    "    time_series_id_column_names=time_series_id_column_names,\n",
    "    aggregation_method=np.median\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c922fc",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"regression\">Regression</a></center></font>\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'y_max', 'y_min', 'y_std', 'bin_info', 'sample_weight', 'enable_metric_confidence', 'confidence_metrics']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed. |\n",
    "| y_max | Maximum target value |\n",
    "| y_min | Minimum target value |\n",
    "| y_std | Standard deviation of the target values. |\n",
    "| bin_info | Dictionary containing bin information for computing calibration metrics. |\n",
    "| sample_weight | Weights for each of the individal samples |\n",
    "| enable_metric_confidence | Boolean to Compute confidence interval for supported metrics (default=False) |\n",
    "| confidence_metrics | List of metrics to compute confidence intervals |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba013d",
   "metadata": {},
   "source": [
    "#### List metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e645465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.REGRESSION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec11c6",
   "metadata": {},
   "source": [
    "#### Testing basic case of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1635c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.REGRESSION, \n",
    "                         y_test=[1,0,1,2,0,2], y_pred=[1,0,1,2,0,1])\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abccb5a",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "metrics_config = {\n",
    "     \"sample_weight\" : [1, 2, 3, 4, 5, 6],\n",
    "     \"y_min\" : 0,\n",
    "     \"y_max\" : 2,\n",
    "     \"dummy_argument\" : \"dummy_value\"\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.REGRESSION, \n",
    "                         y_test=[1,0,1,2,0,2], y_pred=[1,0,1,2,0,1],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82757ee",
   "metadata": {},
   "source": [
    "## Text based tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15728efc",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"chat-completion\">Chat Completion</a></center></font>\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'tokenizer', 'smoothing', 'aggregator', 'stemmer', 'model_id', 'batch_size', 'add_start_token', 'openai_params', 'openai_api_batch_size', 'use_chat_completion_api', 'score_version', 'use_previous_conversation']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| tokenizer | Tokenizer object to perform tokenization on provided input text |\n",
    "| smoothing | Boolean flag to indicate if bleu score needs to be smoothend |\n",
    "| aggregator | Boolean flag to indicate if need to aggregate rouge scores for individual data points (default=True) |\n",
    "| stemmer | Boolean flag to indicate whether to use Porter Stemmer for suffixes (default=False)  |\n",
    "| model_id | model used for calculating Perplexity |\n",
    "| batch_size | the batch size to run texts through the model (default = 16) |\n",
    "| add_start_token | whether to add the start token to the texts, so the perplexity can include the probability of the first word (default = True) |\n",
    "| openai_params | Dictionary containing credentials for openai API (propogated directly to openai APIs). |\n",
    "| openai_api_batch_size | # of prompts to be batched in one API call (applicable only for models with completion API support). |\n",
    "| use_chat_completion_api | Boolean flag to indicate if openai chat completion API needs to be used (default=None) |\n",
    "| score_version | Version of the prompt template to compute rag based metrics (default=\"v1\") |\n",
    "| use_previous_conversation | Boolean flag to indicate if previous conversation needs to be used for computing rag based metrics (default=False) |\n",
    "\n",
    "**Note on how to set use_chat_completion_api:**\n",
    "\n",
    "When use_chat_completion_api is set as None, we try to determine the type of openai API to be used internally based on static rools (might not work in all cases).\n",
    "\n",
    "| openai model/deployment | examples | use_chat_completion_api | Internal details |\n",
    "| :---: | :-----------: | :--: | :--: | \n",
    "| GPT-4 | gpt-4, gpt-4-32k | True | openai chat completion API is used to compute metrics |\n",
    "| GPT-3.5 | gpt-35-turbo, gpt-35-turbo-16k, gpt-35-turbo-instruct | True | openai chat completion API is used to compute metrics |\n",
    "| GPT-3.5 (Legacy models) | text-davinci-002, text-davinci-003, code-davinci-002 | False | openai completions API is used to compute metrics |\n",
    "| GPT-3 | text-ada-001, text-babbage-001, text-curie-001, text-davinci-001 | False | openai completions API is used to compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa00c6",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "\n",
    "supported_chat_metrics = list_metrics(task_type=constants.Tasks.CHAT_COMPLETION)\n",
    "supported_rag_metrics = list_metrics(task_type=constants.Tasks.RAG_EVALUATION)\n",
    "print(\", \".join(supported_chat_metrics | supported_rag_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4e6b2",
   "metadata": {},
   "source": [
    "#### Testing basic case for chat completion based metrics\n",
    "\n",
    "**Note: We compute the bleu/rouge/perplexity scores for last assistant's response in the conversation.**\n",
    "\n",
    "#### <center>Supported Metrics based on provided inputs</center>\n",
    "\n",
    "| Input to azureml-metrics package | Generated Metrics | Notes |\n",
    "| ------ | ------ | --- |\n",
    "| y_pred (role, content) | Perplexity | Only perplexity is computed for last assistant's response  |\n",
    "| openai_params, y_pred (role, content, context - citations) | Perplexity, RAG metrics (Generation, Grounding, Retrieval) | RAG based metrics can be computed as citations are present in the data for every assistant's response |\n",
    "| y_test, y_pred (role, content) | Bleu, Rouge, Perplexity | Metrics are computed for last assistant's response |\n",
    "| openai_params, y_test, y_pred (role, content, context - citations) | Bleu, Rouge, Perplexity, RAG metrics (Generation, Grounding, Retrieval) | All of the Chat + RAG based metrics are computed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "# length of y_test = number of conversations.\n",
    "# The value of each element -- can have multiple ground truth answers \n",
    "# -- which are expected for last assistant turn.\n",
    "y_test = [[\"In Africa, the tallest building is the Carlton Centre\", \"Johannesburg\"], \n",
    "          [\"4\", \"2 + 2 = 4\"]]\n",
    "\n",
    "y_pred = [\n",
    "    # conversation 0\n",
    "    [   \n",
    "        # turn 0 - with role, content as keys\n",
    "        # role - can either be \"user\" or \"assistant\"\n",
    "        {\"role\": \"user\", \"content\": \"What is the tallest building in the world?\"},\n",
    "        # turn 1\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"As of 2021, the Burj Khalifa in Dubai, United Arab Emirates is the \" \\\n",
    "                \"tallest building in the world, standing at a height of 828 meters (2,722 feet). \" \\\n",
    "                \"It was completed in 2010 and has 163 floors. The Burj Khalifa is not only the tallest\" \\\n",
    "                \" building in the world but also holds several other records, such as the highest \" \\\n",
    "                \"occupied floor, highest outdoor observation deck, elevator with the longest travel \" \\\n",
    "                \"distance, and the tallest freestanding structure in the world.\",\n",
    "        },\n",
    "        # turn 2\n",
    "        {\"role\": \"user\", \"content\": \"and in Africa?\"},\n",
    "        # turn 3\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"In Africa, the tallest building is the Carlton Centre, located in \" \\\n",
    "                \"Johannesburg, South Africa. It stands at a height of 50 floors and 223 meters \" \\\n",
    "                \"(730 feet). The CarltonDefault Centre was completed in 1973 and was the tallest \" \\\n",
    "                \"building in Africa for many years until the construction of the Leonardo, a 55-story \" \\\n",
    "                \"skyscraper in Sandton, Johannesburg, which was completed in 2019 and stands at a \" \\\n",
    "                \"height of 230 meters (755 feet). Other notable tall buildings in Africa include the \" \\\n",
    "                \"Ponte City Apartments in Johannesburg, the John Hancock Center in Lagos, Nigeria, \" \\\n",
    "                \"and the Alpha II Building in Abidjan, Ivory Coast\",\n",
    "        },\n",
    "        # turn 4\n",
    "        {\"role\": \"user\", \"content\": \"and in Europe?\"},\n",
    "    ],\n",
    "    # conversation 1\n",
    "    [\n",
    "        # turn 0\n",
    "        {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "        # turn 1\n",
    "        {\"role\": \"assistant\", \"content\": \"2 + 2 = 4\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CHAT_COMPLETION, \n",
    "                         y_test=y_test, y_pred=y_pred)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "# length of y_test = number of conversations.\n",
    "# The value of each element -- can have multiple ground truth answers \n",
    "# -- which are expected for last assistant turn.\n",
    "y_test = [[\"In Africa, the tallest building is the Carlton Centre\", \"Johannesburg\"], \n",
    "          [\"4\", \"2 + 2 = 4\"]]\n",
    "\n",
    "y_pred = [\n",
    "    # conversation 0\n",
    "    [   \n",
    "        # turn 0 - with role, content as keys\n",
    "        # role - can either be \"user\" or \"assistant\"\n",
    "        {\"role\": \"user\", \"content\": \"What is the tallest building in the world?\"},\n",
    "        # turn 1\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"As of 2021, the Burj Khalifa in Dubai, United Arab Emirates is the \" \\\n",
    "                \"tallest building in the world, standing at a height of 828 meters (2,722 feet). \" \\\n",
    "                \"It was completed in 2010 and has 163 floors. The Burj Khalifa is not only the tallest\" \\\n",
    "                \" building in the world but also holds several other records, such as the highest \" \\\n",
    "                \"occupied floor, highest outdoor observation deck, elevator with the longest travel \" \\\n",
    "                \"distance, and the tallest freestanding structure in the world.\",\n",
    "            \"context\": {\n",
    "                \"citations\": \"Documents referenced by the assistant in json format\"\n",
    "            }\n",
    "        },\n",
    "        # turn 2\n",
    "        {\"role\": \"user\", \"content\": \"and in Africa?\"},\n",
    "        # turn 3\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"In Africa, the tallest building is the Carlton Centre, located in \" \\\n",
    "                \"Johannesburg, South Africa. It stands at a height of 50 floors and 223 meters \" \\\n",
    "                \"(730 feet). The CarltonDefault Centre was completed in 1973 and was the tallest \" \\\n",
    "                \"building in Africa for many years until the construction of the Leonardo, a 55-story \" \\\n",
    "                \"skyscraper in Sandton, Johannesburg, which was completed in 2019 and stands at a \" \\\n",
    "                \"height of 230 meters (755 feet). Other notable tall buildings in Africa include the \" \\\n",
    "                \"Ponte City Apartments in Johannesburg, the John Hancock Center in Lagos, Nigeria, \" \\\n",
    "                \"and the Alpha II Building in Abidjan, Ivory Coast\",\n",
    "            \"context\": {\n",
    "                \"citations\": \"Documents referenced by the assistant in json format\"\n",
    "            }\n",
    "        },\n",
    "        # turn 4\n",
    "        {\"role\": \"user\", \"content\": \"and in Europe?\"},\n",
    "    ],\n",
    "    # conversation 1\n",
    "    [\n",
    "        # turn 0\n",
    "        {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "        # turn 1\n",
    "        {\"role\": \"assistant\", \"content\": \"2 + 2 = 4\",\n",
    "         \"context\": {\n",
    "                \"citations\": \"Documents referenced by the assistant in json format\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Note : please replace <placeholder> with actual values.\n",
    "openai_params = {\n",
    "    \"api_version\": \"<placeholder>\",\n",
    "    \"api_base\": \"<placeholder>\",\n",
    "    \"api_type\": \"<placeholder>\",\n",
    "    \"api_key\" : \"<placeholder>\",\n",
    "    \"deployment_id\": \"<placeholder>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "    \"openai_params\" : openai_params,\n",
    "    # set this to True/False based on description above\n",
    "    \"use_chat_completion_api\" : True,\n",
    "    # If we want the model to use previous conversation context set this value to True\n",
    "    # Note: Setting this value to True increases reliability of metrics but might be expensive\n",
    "    \"use_previous_conversation\": False\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CHAT_COMPLETION, \n",
    "                         y_test=y_test, y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5775a",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"code-generation\">Code generation</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'tokenizer', 'smoothing', 'aggregator', 'stemmer', 'test_cases', 'allow_code_eval', 'no_of_candidates', 'num_workers', 'timeout']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| tokenizer | Tokenizer object to perform tokenization on provided input text |\n",
    "| smoothing | Boolean flag to indicate if bleu score needs to be smoothend |\n",
    "| aggregator | Boolean flag to indicate if need to aggregate rouge scores for individual data points (default=True) |\n",
    "| stemmer | Boolean flag to indicate whether to use Porter Stemmer for suffixes (default=False)  |\n",
    "| test_cases | List of strings which contains the assertion or test case statements  |\n",
    "| **allow_code_eval** | Boolean flag to indicate whether to execute untrusted model generated code (default=True)  |\n",
    "| no_of_candidates | List containing number of code candidates to consider in the evaluation. (default=[1, 10, 100])  |\n",
    "| num_workers | number of workers used to evaluate the candidate programs (default=4)  |\n",
    "| timeout | The maximum time taken to produce a prediction before it is considered a “timeout”. (default=3)  |\n",
    "\n",
    "Please find the description about supported arguments for code evaluation:\n",
    "\n",
    "Use Case: Prompt is provided as input to LLM for generating code and we can use the task of code-generation to test this scenario.\n",
    "\n",
    "| Input | Description | Input Format |\n",
    "| ------ | ------ | --- |\n",
    "| y_test | Ideal/Expected code | List of List of strings |\n",
    "|test_cases | Unit test function/Assertion Statement which can be passed using ideal code | List of strings |\n",
    "| y_pred | Code generated by the model (model can generate multiple predictions) | List of list of strings |\n",
    "    \n",
    "#### <center>Supported Metrics by azureml-metrics based on provided inputs</center>\n",
    "\n",
    "| Input to azureml-metrics package | Generated Metrics | Notes |\n",
    "| ------ | ------ | --- |\n",
    "| y_test, y_pred, test_cases | Bleu, Rouge, Pass@k | All the metrics are generated |\n",
    "| y_test, y_pred | Bleu, Rouge | Pass@k will be set as 'not-applicable' |\n",
    "| y_pred, test_cases | Pass@k/code_eval | Bleu, Rouge can't be computed. |\n",
    "\n",
    "\n",
    "**Note: Computation of ```pass@k/code_eval``` metric is not supported in some operating systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af7d93",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.CODE_GENERATION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e24de",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: Sending y_test, y_pred, test_cases\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "# original code - canoncial solution\n",
    "y_test = [[\"def add(a, b): return a+b\"], [\"def multiply(a, b): return a*b\"]]\n",
    "\n",
    "# predicted code - we can have multiple predictions for pass@k metric\n",
    "# for computing bleu/rouge we can use only first value in every list\n",
    "y_pred = [[\"def add(a, b): return a+b\", \"def add(a,b): return a*b\"], [\"def multiply(a, b): return a+b\", \"def multiply(a,b): return a*b\"]]\n",
    "\n",
    "# assertion/unit test cases which ideal code can clear\n",
    "test_cases = [\"assert add(2,3)==5\", \"assert multiply(2,3)==6\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CODE_GENERATION, y_test=y_test, y_pred=y_pred, test_cases=test_cases)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: Sending y_test, y_pred\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "# original code - canoncial solution\n",
    "y_test = [[\"def add(a, b): return a+b\"], [\"def multiply(a, b): return a*b\"]]\n",
    "\n",
    "# predicted code - we can have multiple predictions for pass@k metric\n",
    "# for computing bleu/rouge we can use only first value in every list\n",
    "y_pred = [[\"def add(a, b): return a+b\", \"def add(a,b): return a*b\"], \n",
    "          [\"def multiply(a, b): return a+b\", \"def multiply(a,b): return a*b\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CODE_GENERATION, y_test=y_test, y_pred=y_pred)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900308bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3: Sending y_pred, test_cases\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "# predicted code - we can have multiple predictions for pass@k metric\n",
    "# for computing bleu/rouge we can use only first value in every list\n",
    "y_pred = [[\"def add(a, b): return a+b\", \"def add(a,b): return a*b\"], \n",
    "          [\"def multiply(a, b): return a+b\", \"def multiply(a,b): return a*b\"]]\n",
    "\n",
    "# assertion/unit test cases which ideal code can clear\n",
    "test_cases = [\"assert add(2,3)==5\", \"assert multiply(2,3)==6\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CODE_GENERATION, y_pred=y_pred, test_cases=test_cases)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c34b16",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ef624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "# predicted code - we can have multiple predictions for pass@k metric\n",
    "# for computing bleu/rouge we can use only first value in every list\n",
    "y_pred = [[\"def add(a, b): return a+b\", \"def add(a,b): return a*b\"], \n",
    "          [\"def multiply(a, b): return a+b\", \"def multiply(a,b): return a*b\"]]\n",
    "\n",
    "# assertion/unit test cases which ideal code can clear\n",
    "test_cases = [\"assert add(2,3)==5\", \"assert multiply(2,3)==6\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CODE_GENERATION, y_pred=y_pred, \n",
    "                         test_cases=test_cases, new_keyword_argument=True)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73b00f",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_test = [[\"def add(a, b): return a+b\"], [\"def multiply(a, b): return a*b\"]]\n",
    "y_pred = [[\"def add(a, b): return a+b\", \"def add(a,b): return a*b\"], \n",
    "          [\"def multiply(a, b): return a+b\", \"def multiply(a,b): return a*b\"]]\n",
    "\n",
    "test_cases = [\"assert add(2,3)==5\", \"assert add(2,3)==5\"]\n",
    "\n",
    "metrics_config = {\n",
    "    \"test_cases\" : test_cases,\n",
    "    \"no_of_candidates\" : [1, 2],\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.CODE_GENERATION, \n",
    "                         y_test=y_test, y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091fd21",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"fill-mask\">Fill Mask</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'model_id', 'batch_size', 'add_start_token']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| model_id | model used for calculating Perplexity |\n",
    "| batch_size | the batch size to run texts through the model (default = 16) |\n",
    "| add_start_token | whether to add the start token to the texts, so the perplexity can include the probability of the first word (default = True) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3389462",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa666ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.FILL_MASK)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c68389",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef363ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "y_pred = [\"hi\", \"green and blue\", \"he dances\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.FILL_MASK, y_pred=y_pred,\n",
    "                model_id=\"gpt2\")\n",
    "\n",
    "pprint(result)\n",
    "\n",
    "mean_perplexity = np.mean(result[\"artifacts\"][\"perplexities\"])\n",
    "\n",
    "print(f\"Mean perplexity : {mean_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b4c96",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab165fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "y_pred = [\"hi\", \"green and blue\", \"he dances\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.FILL_MASK, y_pred=y_pred, ignore_punctuation=True,\n",
    "                model_id=\"gpt2\")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f717da",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85437284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "metrics_config = {\n",
    "    \"model_id\" : \"gpt2\",\n",
    "    \"add_start_token\": True,\n",
    "}\n",
    "\n",
    "y_pred = [\"have a good day\", \"ram is a good boy\", \"we are computing metrics\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.FILL_MASK, y_pred=y_pred, **metrics_config)\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3307c",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"named_entity_recognition\">Named Entity Recogniton</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2966cd",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.TEXT_NER)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fe744",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b4437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "metrics_obj = compute_metrics(task_type=\"text-ner\", y_test=y_true, y_pred=y_pred)\n",
    "pprint(metrics_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced4e83",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417c955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "metrics_obj = compute_metrics(task_type=\"text-ner\", y_test=y_true, y_pred=y_pred, multilabel=True)\n",
    "pprint(metrics_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cb455",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"qa\">Question Answering</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'tokenizer', 'regexes_to_ignore', 'ignore_case', 'ignore_punctuation', 'ignore_numbers', 'lang', 'model_type', 'questions', 'openai_params', 'idf', 'rescale_with_baseline', 'contexts', 'openai_api_batch_size', 'llm_params', 'llm_api_batch_size', 'use_chat_completion_api']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| tokenizer | Tokenizer object to perform tokenization on provided input text |\n",
    "| regexes_to_ignore | List of regex to ignore in our input data points |\n",
    "| ignore_case | Boolean flag to indicate whether we need to ignore case (default=False)  |\n",
    "| ignore_punctuation | Boolean flag to indicate whether we need to ignore punctuation (default=False) |\n",
    "| ignore_numbers | Boolean flag to indicate whether we need to ignore numbers (default=False)  |\n",
    "| lang | String of two letters indicating the language of the sentences, in ISO 639-1 format. (default=\"en\") |\n",
    "| model_type | String specifying which model to use, according to the BERT specification. (default=\"microsoft/deberta-large\") |\n",
    "| idf | Boolean flag to use idf weights during computation of BERT score. (default=False) |\n",
    "| rescale_with_baseline | Boolean flag to rescale BERTScore with the pre-computed baseline. (default=True)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03812ba6",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a92e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.QUESTION_ANSWERING)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07e075",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17689bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi 123\",\"foo bar foobar\", \"ram 234\", \"sid\"]\n",
    "y_test = [\"hello there general kenobi san\", \"foo bar foobar\", \"ram 23\", \"sid$\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, y_test=y_test, y_pred=y_pred)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ec145",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6700fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi 123\",\"foo bar foobar\", \"ram 234\", \"sid\"]\n",
    "y_test = [\"hello there general kenobi san\", \"foo bar foobar\", \"ram 23\", \"sid$\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, y_test=y_test, y_pred=y_pred, multilabel=True)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78bffd",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee4def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi 123\",\"foo bar foobar\", \"ram 234\", \"sid\"]\n",
    "y_test = [\"hello there general kenobi san\", \"foo bar foobar\", \"ram 23\", \"sid$\"]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, y_test=y_test, y_pred=y_pred,\n",
    "                         ignore_numbers=True, ignore_punctuation=True)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613a069",
   "metadata": {},
   "source": [
    "#### <a id=\"gpt-star\">Computing {GPT/LLM}-Star metrics</a>\n",
    "\n",
    "Keyword Arguments for computing {GPT/LLM}-Star metrics:\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| contexts | List of context information used for computing gpt-related metrics. |\n",
    "| questions | List of questions used for the data sample used in computation of gpt-related metrics. |\n",
    "| openai_params | Dictionary containing credentials for openai API (propogated directly to openai APIs). |\n",
    "| openai_api_batch_size | # of prompts to be batched in one API call (applicable only for models with completion API support). |\n",
    "| llm_params | Dictionary containing credentials for openai API (propogated directly to openai APIs). |\n",
    "| llm_api_batch_size | # of prompts to be batched in one API call |\n",
    "| use_chat_completion_api | Boolean flag to indicate if openai chat completion API needs to be used (default=None) |\n",
    "\n",
    "\n",
    "**Note on how to set use_chat_completion_api:**\n",
    "\n",
    "When use_chat_completion_api is set as None, we try to determine the type of openai API to be used internally based on static rools (might not work in all cases).\n",
    "\n",
    "| openai model/deployment | examples | use_chat_completion_api | Internal details |\n",
    "| :---: | :-----------: | :--: | :--: | \n",
    "| GPT-4 | gpt-4, gpt-4-32k | True | openai chat completion API is used to compute metrics |\n",
    "| GPT-3.5 | gpt-35-turbo, gpt-35-turbo-16k, gpt-35-turbo-instruct | True | openai chat completion API is used to compute metrics |\n",
    "| GPT-3.5 (Legacy models) | text-davinci-002, text-davinci-003, code-davinci-002 | False | openai completions API is used to compute metrics |\n",
    "| GPT-3 | text-ada-001, text-babbage-001, text-curie-001, text-davinci-001 | False | openai completions API is used to compute metrics\n",
    "\n",
    "Note: For BERTScore, GPT-Star metrics we compute the results for every data sample and return as part of \"artifacts\" key of compute_metrics() API result.\n",
    "\n",
    "Please find the details about supported GPT-Star metrics and their required inputs here:\n",
    "\n",
    "| Metric             | Required Inputs (kwargs name as supported by azureml-metrics)  | # of tokens                          |\n",
    "| -------------------| --------------------------------------------- | ---------------------------------------------------- |\n",
    "| GPT-Coherence      | Question, Prediction (questions, y_pred)      | 434 + (# of tokens in required inputs) + 1 (token for output) |\n",
    "| GPT-Fluency        | Question, Prediction (questions, y_pred)      | 415 + (# of tokens in required inputs) + 1 (token for output) |\n",
    "| GPT-Relevancy      | Context, Question, Prediction (contexts, questions, y_pred) | 621 + (# of tokens in required inputs) + 1 (token for output) |\n",
    "| GPT-Groundedness   | Context, Prediction (contexts, y_pred)        | 620 + (# of tokens in required inputs) + 1 (token for output) |\n",
    "| GPT-Similarity     | Question, Ground-Truth, Prediction (questions, y_test, y_pred) | 801 + (# of tokens in required inputs) + 1 (token for output) |\n",
    "\n",
    "**Note: Based on the inputs provided, azureml-metrics package computes all supported GPT-Star Metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# this dictionary is propogated to openai completion or chat completion API.\n",
    "# please add the keys directly accepted by openai API.\n",
    "\n",
    "# Note : please replace <placeholder> with actual values.\n",
    "openai_params = {\n",
    "    \"api_version\": \"<placeholder>\",\n",
    "    \"api_base\": \"<placeholder>\",\n",
    "    \"api_type\": \"<placeholder>\",\n",
    "    \"api_key\" : \"<placeholder>\",\n",
    "    \"deployment_id\": \"<placeholder>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "     \"questions\" : [question, question],\n",
    "     \"contexts\" : [context, context],\n",
    "     \"openai_params\" : openai_params\n",
    "}\n",
    "\n",
    "# Note : length of lists of y_test, y_pred, questions, contexts should be equal\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, \n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bf9d3",
   "metadata": {},
   "source": [
    "#### Computing one of the quality metrics (Example: GPT-Similarity)\n",
    "\n",
    "Note: As discussed previously, azureml-metrics package computes all supported GPT-Star Metrics based on provided inputs. To compute one or subset of metrics please use ```metrics``` keyword argument.\n",
    "\n",
    "Example: Using ```metrics``` keyword argument to compute subset of metrics:\n",
    "\n",
    "| Metric             | metrics keyword argument  | \n",
    "| -------------------| ----------------------------- | \n",
    "| GPT-Coherence      | metrics=[\"gpt_coherence\"]    | \n",
    "| GPT-Coherence, GPT-Fluency | metrics=[\"gpt_coherence\", \"gpt_fluency\"] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc301a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# this dictionary is propogated to openai completion or chat completion API.\n",
    "# please add the keys directly accepted by openai API.\n",
    "\n",
    "# Note : please replace <placeholder> with actual values.\n",
    "openai_params = {\n",
    "    \"api_version\": \"<placeholder>\",\n",
    "    \"api_base\": \"<placeholder>\",\n",
    "    \"api_type\": \"<placeholder>\",\n",
    "    \"api_key\" : \"<placeholder>\",\n",
    "    \"deployment_id\": \"<placeholder>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "     \"questions\" : [question, question],\n",
    "    # we can remove context as it is not needed for computing gpt-similarity metric\n",
    "    # \"contexts\" : [context, context],\n",
    "     \"openai_params\" : openai_params,\n",
    "     \"metrics\": [\"gpt_similarity\"],\n",
    "}\n",
    "\n",
    "# Note : length of lists of y_test, y_pred, questions, contexts should be equal\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, \n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d3cab",
   "metadata": {},
   "source": [
    "#### Computing LLM Star metrics\n",
    "\n",
    "Note: To compute LLM Star metrics we need llm_params through which we can make a http request to LLM API.\n",
    "\n",
    "Also, all the details about GPT based metrics can be extended for LLM based metrics. GPT/LLM star metrics for same dataset can also be computed in a single API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c44302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# Note: we will make a http request to this endpoint to compute the metrics\n",
    "# Note: Please replace the values for the following variables with your own values.\n",
    "llm_params = {\n",
    "    \"llm_url\": \"<rest_endpoint_after_deployment_from_azureml_model_catalog>\",\n",
    "    \"llm_api_key\": \"<api_key_for_endpoint>\",\n",
    "    \"azureml_model_deployment\": \"<deployment_name>\",\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "    \"llm_params\": llm_params,\n",
    "    \"questions\": [question, question],\n",
    "    \"contexts\": [context, context],\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING,\n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing both GPT/LLM metrics in a single compute_metrics call\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# Note: we will make a http request to this endpoint to compute the metrics\n",
    "llm_params = {\n",
    "    \"llm_url\": \"<deployed_endpoint_url_of_llm_model>\",\n",
    "    \"llm_api_key\": \"<api_key_of_llm_model>\",\n",
    "}\n",
    "\n",
    "# Note : please replace <placeholder> with actual values.\n",
    "openai_params = {\n",
    "    \"api_version\": \"<placeholder>\",\n",
    "    \"api_base\": \"<placeholder>\",\n",
    "    \"api_type\": \"<placeholder>\",\n",
    "    \"api_key\" : \"<placeholder>\",\n",
    "    \"deployment_id\": \"<placeholder>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "    \"openai_params\": openai_params,\n",
    "    \"llm_params\": llm_params,\n",
    "    \"questions\": [question, question],\n",
    "    \"contexts\": [context, context],\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING,\n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7979b03",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"summarization\">Summarization</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'tokenizer', 'aggregator', 'stemmer']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| tokenizer | Tokenizer object to perform tokenization on provided input text |\n",
    "| aggregator | Boolean flag to indicate if need to aggregate rouge scores for individual data points (default=True) |\n",
    "| stemmer | Boolean flag to indicate whether to use Porter Stemmer for suffixes (default=False)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234fa45",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1844a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.SUMMARIZATION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5907b1f",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7246e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.SUMMARIZATION, y_test=y_test, y_pred=y_pred)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbb7e3",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306f33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.SUMMARIZATION, y_test=y_test, y_pred=y_pred, multilabel=True)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99b82c",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4587b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.SUMMARIZATION, y_test=y_test, y_pred=y_pred, \n",
    "                         multilabel=True, stemmer=True, aggregator=False)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91815f3c",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"text-generation\">Text generation</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'tokenizer', 'smoothing', 'aggregator', 'stemmer', 'model_id', 'batch_size', 'add_start_token']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| tokenizer | Tokenizer object to perform tokenization on provided input text |\n",
    "| smoothing | Boolean flag to indicate if bleu score needs to be smoothend |\n",
    "| aggregator | Boolean flag to indicate if need to aggregate rouge scores for individual data points (default=True) |\n",
    "| stemmer | Boolean flag to indicate whether to use Porter Stemmer for suffixes (default=False)  |\n",
    "| model_id | model used for calculating Perplexity |\n",
    "| batch_size | the batch size to run texts through the model (default = 16) |\n",
    "| add_start_token | whether to add the start token to the texts, so the perplexity can include the probability of the first word (default = True) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6f452",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38253386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.TEXT_GENERATION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73281fc5",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415deb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\", \"blue & red\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"], [\"blue & green\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.TEXT_GENERATION, y_test=y_test, y_pred=y_pred)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02609f",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\", \"blue & red\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"], [\"blue & green\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.TEXT_GENERATION, y_test=y_test, y_pred=y_pred, new_keyword_argument=True)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74f76a",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\", \"blue & red\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"], [\"blue & green\"]]\n",
    "\n",
    "tokenizer = lambda text : text.split()\n",
    "\n",
    "metrics_config = {\n",
    "    \"smoothing\" : True,\n",
    "    \"aggregator\" : False,\n",
    "    \"tokenizer\" : tokenizer,\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.TEXT_GENERATION, y_test=y_test, y_pred=y_pred,**metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35bb52",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"translation\">Translation</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'tokenizer', 'smoothing']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| tokenizer | Tokenizer object to perform tokenization on provided input text |\n",
    "| smoothing | Boolean flag to indicate if bleu score needs to be smoothend |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31c12e",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc22f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.TRANSLATION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb64668",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0db65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.TRANSLATION, y_test=y_test, y_pred=y_pred)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c947f3e",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc3b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.TRANSLATION, y_test=y_test, y_pred=y_pred, multilabel=True)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda298e",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360e8fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
    "y_test = [[\"hello there general kenobi san\"], [\"foo bar foobar\"]]\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.TRANSLATION, y_test=y_test, y_pred=y_pred, \n",
    "                         tokenizer=lambda x : x.split(),\n",
    "                         smoothing=True)\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cc707",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"image-obj-detection\">Image Object Detction</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'num_classes', 'iou_threshold', 'image_meta_info']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| num_classes | The number of classes in the dataset |\n",
    "| iou_threshold | IOU threshold used when matching ground truth objects with predicted objects |\n",
    "| image_meta_info | Meta information for each image. list of dict's that have \"iscrowd\" key |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1a7c2",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.IMAGE_OBJECT_DETECTION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e69ec",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73aa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"areas\": [60000],\n",
    "        \"iscrowd\": [0],\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"classes\": np.array([1]),\n",
    "    }\n",
    "]\n",
    "predicted_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": None,\n",
    "        \"classes\": np.array([1]),\n",
    "        \"scores\": np.array([0.75]),\n",
    "    }\n",
    "]\n",
    "result = compute_metrics(\n",
    "    metrics = supported_metrics,\n",
    "    task_type=constants.Tasks.IMAGE_OBJECT_DETECTION,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=3,\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cadead",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"areas\": [60000],\n",
    "        \"iscrowd\": [0],\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"classes\": np.array([1]),\n",
    "    }\n",
    "]\n",
    "predicted_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": None,\n",
    "        \"classes\": np.array([1]),\n",
    "        \"scores\": np.array([0.75]),\n",
    "    }\n",
    "]\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.IMAGE_OBJECT_DETECTION,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=3,\n",
    "    new_keyword=True\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047b1b5",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"areas\": [60000],\n",
    "        \"iscrowd\": [0],\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"classes\": np.array([1]),\n",
    "    }\n",
    "]\n",
    "predicted_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": None,\n",
    "        \"classes\": np.array([1]),\n",
    "        \"scores\": np.array([0.75]),\n",
    "    }\n",
    "]\n",
    "metrics_config = {\n",
    "    'iou_threshold' : 0.5\n",
    "}\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.IMAGE_OBJECT_DETECTION,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=3,\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aaf0da",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"image-instance-segmentation\">Image Instance Segmentation</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'num_classes', 'iou_threshold', 'image_meta_info']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| num_classes | The number of classes in the dataset |\n",
    "| iou_threshold | IOU threshold used when matching ground truth objects with predicted objects |\n",
    "| image_meta_info | Meta information for each image. list of dict's that have \"iscrowd\" key |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28591d7",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.IMAGE_OBJECT_DETECTION)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ddc94",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pycocotools import mask as pycoco_mask\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def _get_mask_from_bbox(bbox: List, height: int, width: int) -> Dict[str, Any]:\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    polygon = [[x1, y1, x2, y1, x2, y2, x1, y2, x1, y1]]\n",
    "    rle_masks = pycoco_mask.frPyObjects(polygon, height, width)\n",
    "    return rle_masks[0]\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"areas\": [60000],\n",
    "        \"iscrowd\": [0],\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": [_get_mask_from_bbox([160, 120, 320, 240], 640, 640)],\n",
    "        \"classes\": np.array([1]),\n",
    "    }\n",
    "]\n",
    "predicted_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": [_get_mask_from_bbox([160, 120, 320, 240], 640, 640)],\n",
    "        \"classes\": np.array([1]),\n",
    "        \"scores\": np.array([0.75]),\n",
    "    }\n",
    "]\n",
    "result = compute_metrics(\n",
    "    metrics = supported_metrics,\n",
    "    task_type=constants.Tasks.IMAGE_OBJECT_DETECTION,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=3,\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01d16c",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fc668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"areas\": [60000],\n",
    "        \"iscrowd\": [0],\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": [_get_mask_from_bbox([160, 120, 320, 240], 640, 640)],\n",
    "        \"classes\": np.array([1]),\n",
    "    }\n",
    "]\n",
    "predicted_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": [_get_mask_from_bbox([160, 120, 320, 240], 640, 640)],\n",
    "        \"classes\": np.array([1]),\n",
    "        \"scores\": np.array([0.75]),\n",
    "    }\n",
    "]\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.IMAGE_OBJECT_DETECTION,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=3,\n",
    "    new_keyword=True\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c9cb6",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"areas\": [60000],\n",
    "        \"iscrowd\": [0],\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": [_get_mask_from_bbox([160, 120, 320, 240], 640, 640)],\n",
    "        \"classes\": np.array([1]),\n",
    "    }\n",
    "]\n",
    "predicted_objects_per_image = [\n",
    "    {\n",
    "        \"boxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"masks\": [_get_mask_from_bbox([160, 120, 320, 240], 640, 640)],\n",
    "        \"classes\": np.array([1]),\n",
    "        \"scores\": np.array([0.75]),\n",
    "    }\n",
    "]\n",
    "metrics_config = {\n",
    "    'iou_threshold' : 0.5\n",
    "}\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.IMAGE_OBJECT_DETECTION,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=3,\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98996d4e",
   "metadata": {},
   "source": [
    "### <font color=\"green\"><center><a id=\"video-multi-object-tracking\">Video Multi-Object Tracking</a></center></font>\n",
    "\n",
    "\n",
    "List of supported keyword arguments:\n",
    "\n",
    "['metrics', 'num_classes', 'iou_threshold', 'image_meta_info']\n",
    "\n",
    "| Keyword Argument | Description |\n",
    "| :---: | :----------- |\n",
    "| metrics | List for subset of metrics to be computed |\n",
    "| num_classes | The number of classes in the dataset |\n",
    "| iou_threshold | IOU threshold used when matching ground truth objects with predicted objects |\n",
    "| image_meta_info | Meta information for each image. list of dict's that have \"frame_id\" key |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e0c29",
   "metadata": {},
   "source": [
    "#### Package Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pip==21.1.1\n",
    "! pip install azureml-metrics[video]\n",
    "! pip install mmtrack==0.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a484fc",
   "metadata": {},
   "source": [
    "#### List of supported metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "supported_metrics = list_metrics(task_type=constants.Tasks.VIDEO_MULTI_OBJECT_TRACKING)\n",
    "print(\", \".join(supported_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419923de",
   "metadata": {},
   "source": [
    "#### Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ca2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"frame_id\": 0,\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = {\n",
    "        \"bboxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"labels\": np.array([0]),\n",
    "        \"instance_ids\": np.array([1]),\n",
    "        \"bboxes_ignore\": np.zeros((0, 4), dtype=np.float32)\n",
    "    }\n",
    "\n",
    "predicted_objects_per_image = [\n",
    "    np.array([[0, 160, 120, 320, 240, 0.75]], dtype=np.float32)]\n",
    "\n",
    "num_classes = 1\n",
    "iou_threshold = 0.5\n",
    "\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.VIDEO_MULTI_OBJECT_TRACKING,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=num_classes,\n",
    "    iou_threshold=iou_threshold,\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057752cb",
   "metadata": {},
   "source": [
    "#### Getting hold of ununsed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f590a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"frame_id\": 0,\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = {\n",
    "        \"bboxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"labels\": np.array([0]),\n",
    "        \"instance_ids\": np.array([1]),\n",
    "        \"bboxes_ignore\": np.zeros((0, 4), dtype=np.float32)\n",
    "    }\n",
    "\n",
    "predicted_objects_per_image = [\n",
    "    np.array([[0, 160, 120, 320, 240, 0.75]], dtype=np.float32)]\n",
    "\n",
    "num_classes = 1\n",
    "iou_threshold = 0.5\n",
    "\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.VIDEO_MULTI_OBJECT_TRACKING,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    num_classes=num_classes,\n",
    "    iou_threshold=iou_threshold,\n",
    "    new_keyword=True\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b6c0d",
   "metadata": {},
   "source": [
    "#### Trying few keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddd26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "meta_info_per_image = [\n",
    "    {\n",
    "        \"filename\": \"image_1.jpg\",\n",
    "        \"frame_id\": 0,\n",
    "        \"height\": 640,\n",
    "        \"width\": 480,\n",
    "        \"original_width\": 640,\n",
    "        \"original_height\": 480,\n",
    "    }\n",
    "]\n",
    "gt_objects_per_image = {\n",
    "        \"bboxes\": np.array([[160, 120, 320, 240]], dtype=np.float32),\n",
    "        \"labels\": np.array([0]),\n",
    "        \"instance_ids\": np.array([1]),\n",
    "        \"bboxes_ignore\": np.zeros((0, 4), dtype=np.float32)\n",
    "    }\n",
    "\n",
    "predicted_objects_per_image = [\n",
    "    np.array([[0, 160, 120, 320, 240, 0.75]], dtype=np.float32)]\n",
    "\n",
    "metrics_config = {\n",
    "    'iou_threshold' : 0.5,\n",
    "    'num_classes': 1\n",
    "}\n",
    "result = compute_metrics(\n",
    "    task_type=constants.Tasks.VIDEO_MULTI_OBJECT_TRACKING,\n",
    "    y_test=gt_objects_per_image,\n",
    "    image_meta_info=meta_info_per_image,\n",
    "    y_pred=predicted_objects_per_image,\n",
    "    **metrics_config\n",
    ")\n",
    "pprint(result[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab2702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
