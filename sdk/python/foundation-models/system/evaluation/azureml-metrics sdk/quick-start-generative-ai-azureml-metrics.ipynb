{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative AI Metrics\n",
    "\n",
    "1) GPT Star Metrics - Question Answering\n",
    "2) RAG Evaluation Metrics - Chat Completion\n",
    "\n",
    "#### Appendix:\n",
    "\n",
    "3) Computation of other Question Answering Metrics\n",
    "4) Computation of other Chat Completion Metrics\n",
    "\n",
    "For more details please refer to the quick start guide here: https://aka.ms/azureml-metrics-quick-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run this notebook\n",
    "\n",
    "* Creating a conda environment\n",
    "\n",
    "```$ conda create --name <env_name> python=3.8```\n",
    "\n",
    "* Deleting a conda environment\n",
    "\n",
    "```$ conda env remove -n <env_name>```\n",
    "\n",
    "* Activating the environment\n",
    "\n",
    "```$ conda activate <env_name> ```\n",
    "\n",
    "* Please install azureml-metrics package\n",
    "\n",
    "```$ pip install azureml-metrics```\n",
    "\n",
    "- The above command install numpy, pandas, psutil dependencies.\n",
    "\n",
    "- For computing gpt-star, rag based metrics please run:\n",
    "\n",
    "```$ pip install azureml-metrics[generative-ai]```\n",
    "\n",
    "- The above command installs : ('openai', 'tenacity', 'evaluate', \"rtoml\", \"azure-keyvault\", \"azure-identity\", \"requests\", \"aiohttp\") dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Computing GPT-Star Metrics\n",
    "\n",
    "For computing gpt-star metrics we need openai, tenacity libraries. To install the required dependencies we can run:\n",
    "\n",
    "```$ pip install azureml-metrics[generative-ai]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Answering metrics: {'llm_fluency', 'llm_relevance', 'gpt_fluency', 'gpt_groundedness', 'f1_score', 'bertscore', 'llm_similarity', 'gpt_coherence', 'llm_coherence', 'llm_groundedness', 'gpt_relevance', 'gpt_similarity', 'exact_match', 'ada_similarity'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "\n",
    "qa_metrics = list_metrics(task_type=constants.Tasks.QUESTION_ANSWERING)\n",
    "print(\"Question Answering metrics:\", qa_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM related metrics need llm_params to be computed. Computing metrics for ['gpt_fluency', 'gpt_groundedness', 'gpt_coherence', 'gpt_relevance', 'gpt_similarity']\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'gpt_coherence': [5, 3],\n",
      "               'gpt_fluency': [5, 3],\n",
      "               'gpt_groundedness': [5, 3],\n",
      "               'gpt_relevance': [5, 1],\n",
      "               'gpt_similarity': [5, 1]},\n",
      " 'metrics': {'mean_gpt_coherence': 4.0,\n",
      "             'mean_gpt_fluency': 4.0,\n",
      "             'mean_gpt_groundedness': 4.0,\n",
      "             'mean_gpt_relevance': 3.0,\n",
      "             'mean_gpt_similarity': 3.0,\n",
      "             'median_gpt_coherence': 4.0,\n",
      "             'median_gpt_fluency': 4.0,\n",
      "             'median_gpt_groundedness': 4.0,\n",
      "             'median_gpt_relevance': 3.0,\n",
      "             'median_gpt_similarity': 3.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# this dictionary is propogated to openai completion or chat completion API.\n",
    "# please add the keys directly accepted by openai API.\n",
    "openai_params = {\n",
    "    \"api_version\": os.environ[\"OPENAI_API_VERSION\"],\n",
    "    \"api_base\": os.envrion[\"OPENAI_API_BASE\"],\n",
    "    \"api_type\": os.envrion[\"OPENAI_API_TYPE\"],\n",
    "    \"api_key\" : os.envrion[\"OPENAI_API_KEY\"],\n",
    "    \"deployment_id\": \"<deployment_id>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "     \"questions\" : [question, question],\n",
    "     \"contexts\" : [context, context],\n",
    "     \"openai_params\" : openai_params,\n",
    "     # To compute gpt-star metrics\n",
    "     \"metrics\" : [\"gpt_coherence\", \"gpt_fluency\", \"gpt_groundedness\", \"gpt_relevance\", \"gpt_similarity\"]\n",
    "}\n",
    "\n",
    "# Note : length of lists of y_test, y_pred, questions, contexts should be equal\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, \n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Computing RAG based metrics\n",
    "\n",
    "For computing rag based metrics we need openai, tenacity, requests, aiohttp, rtoml, azure-identity, azure-keyvalut libraries. To install the required dependencies we can run:\n",
    "\n",
    "```$ pip install azureml-metrics[generative-ai]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Evaluation based metrics: {'gpt_groundedness', 'gpt_relevance', 'gpt_retrieval_score'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "\n",
    "rag_metrics = list_metrics(task_type=constants.Tasks.RAG_EVALUATION)\n",
    "print(\"RAG Evaluation based metrics:\", rag_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion metrics: {'bleu_4', 'rouge1', 'bleu_2', 'gpt_groundedness', 'rougeL', 'rougeLsum', 'conversation_groundedness_score', 'bleu_1', 'rouge2', 'bleu_3', 'gpt_retrieval_score', 'gpt_relevance', 'perplexity'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import list_metrics, constants\n",
    "\n",
    "chat_metrics = list_metrics(task_type=constants.Tasks.CHAT_COMPLETION)\n",
    "print(\"Chat Completion metrics:\", chat_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gpt relevance score: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "Computing gpt groundedness score: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]\n",
      "Computing gpt retrieval score: 100%|██████████| 2/2 [00:03<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'gpt_groundedness': {'reason': [['<Quality reasoning:> The '\n",
      "                                                'chatbot\\'s response \"What is '\n",
      "                                                'the value of 2 + 2?\" is a '\n",
      "                                                'direct paraphrase of the '\n",
      "                                                'question \"2 + 2 = 4\". The '\n",
      "                                                'factual information in the '\n",
      "                                                \"chatbot's response, which is \"\n",
      "                                                'the equation \"2 + 2 = 4\", is '\n",
      "                                                'directly taken from the '\n",
      "                                                'retrieved document '\n",
      "                                                '\"math_document1.md\" where it '\n",
      "                                                'states \"2 + 2 = 4\". '\n",
      "                                                \"Therefore, the chatbot's \"\n",
      "                                                'response is grounded in the '\n",
      "                                                'retrieved documents.\\n'\n",
      "                                                '<Quality score: 5/5>\\n'\n",
      "                                                '<Input for Labeling End>'],\n",
      "                                               ['<Quality reasoning:> The '\n",
      "                                                \"chatbot's response is \"\n",
      "                                                'directly taken from the '\n",
      "                                                'retrieved document, which '\n",
      "                                                'states that Taj Mahal is '\n",
      "                                                'located in Agra, India.\\n'\n",
      "                                                '<Quality score: 5/5>\\n'\n",
      "                                                '<Input for Labeling End>']],\n",
      "                                    'score_per_conversation': [5.0, 5.0],\n",
      "                                    'score_per_turn': [[5], [5]]},\n",
      "               'gpt_relevance': {'reason': [['Quality score reasoning: The '\n",
      "                                             'provided response is a direct '\n",
      "                                             'and correct answer to the '\n",
      "                                             'question. It includes all the '\n",
      "                                             'necessary information based on '\n",
      "                                             'the conversation history and the '\n",
      "                                             'retrieved document.'],\n",
      "                                            ['Quality score reasoning: The '\n",
      "                                             'provided response is ideal as it '\n",
      "                                             'includes all the necessary '\n",
      "                                             'information to answer the '\n",
      "                                             'question based on the '\n",
      "                                             'conversation history and the '\n",
      "                                             'retrieved document.']],\n",
      "                                 'score_per_conversation': [5.0, 5.0],\n",
      "                                 'score_per_turn': [[5], [5]]},\n",
      "               'gpt_retrieval_score': {'reason': [['# Summarized Documents\\n'\n",
      "                                                   '- Document 1: Information '\n",
      "                                                   'about additions: 1 + 2 = '\n",
      "                                                   '3, 2 + 2 = 4\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Intent of the Question\\n'\n",
      "                                                   'The question is asking for '\n",
      "                                                   'the value of 2 + 2.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Document Relevance '\n",
      "                                                   'Scores\\n'\n",
      "                                                   '- Document 1 '\n",
      "                                                   '(math_document1.md): 5 '\n",
      "                                                   '(contains the exact value '\n",
      "                                                   'of 2 + 2)\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Overall Relevance\\n'\n",
      "                                                   'The document with ID '\n",
      "                                                   '\"math_document1.md\" is '\n",
      "                                                   'highly relevant to the '\n",
      "                                                   'given question as it '\n",
      "                                                   'directly provides the '\n",
      "                                                   'value of 2 + 2.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Result\\n'\n",
      "                                                   '5'],\n",
      "                                                  ['# Summarized Documents\\n'\n",
      "                                                   '- Document 1: Taj Mahal is '\n",
      "                                                   'located in Agra, India and '\n",
      "                                                   'is one of the seven '\n",
      "                                                   'wonders of the world.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Intent of the Question\\n'\n",
      "                                                   'The question is asking for '\n",
      "                                                   'the location of the Taj '\n",
      "                                                   'Mahal.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Document Relevance '\n",
      "                                                   'Scores\\n'\n",
      "                                                   '- Document 1 '\n",
      "                                                   '(taj_mahal_document1.md): '\n",
      "                                                   '5\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Overall Relevance\\n'\n",
      "                                                   'The given document '\n",
      "                                                   '(Document 1) provides the '\n",
      "                                                   'exact location of the Taj '\n",
      "                                                   'Mahal, making it highly '\n",
      "                                                   'relevant to the question.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Result\\n'\n",
      "                                                   '5']],\n",
      "                                       'score_per_conversation': [5.0, 5.0],\n",
      "                                       'score_per_turn': [[5], [5]]}},\n",
      " 'metrics': {'mean_gpt_groundedness': 5.0,\n",
      "             'mean_gpt_relevance': 5.0,\n",
      "             'mean_gpt_retrieval_score': 5.0}}\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpt4-32k model\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "y_test = [[\"4\", \"2 + 2 = 4\"], [\"Agra\", \"Agra, India\"]]\n",
    "\n",
    "y_pred = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"2 + 2 = 4\",\n",
    "         \"context\": {\n",
    "             \"citations\": [{'id': 'math_document1.md',\n",
    "                            'content': 'Information about additions: ' \\\n",
    "                            '1 + 2 = 3, 2 + 2 = 4'}]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Where is Taj Mahal located?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Taj Mahal is located in Agra, India\",\n",
    "         \"context\": {\n",
    "             \"citations\": [{'id': 'taj_mahal_document1.md',\n",
    "                            'content': 'Taj Mahal is located in Agra, India ' \\\n",
    "                                        'and is one of the seven wonders of the world.'}]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "openai_params = {\n",
    "    \"api_version\": os.environ[\"OPENAI_API_VERSION\"],\n",
    "    \"api_base\": os.envrion[\"OPENAI_API_BASE\"],\n",
    "    \"api_type\": os.envrion[\"OPENAI_API_TYPE\"],\n",
    "    \"api_key\" : os.envrion[\"OPENAI_API_KEY\"],\n",
    "    \"deployment_id\": \"<deployment_id>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "    \"openai_params\": openai_params,\n",
    "    \"score_version\": \"v1\",\n",
    "    \"use_chat_completion_api\": True,\n",
    "    # To compute RAG based metrics\n",
    "    \"metrics\": [\"gpt_relevance\", \"gpt_groundedness\", \"gpt_retrieval_score\"]\n",
    "}\n",
    "\n",
    "# The above metrics can even be computed by setting the task_type to RAG_EVALUATION\n",
    "result = compute_metrics(task_type=constants.Tasks.CHAT_COMPLETION,\n",
    "                         y_test=y_test, \n",
    "                         y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Appendix:\n",
    "\n",
    "3) Computing all Question Answering metrics\n",
    "\n",
    "| Metrics | Extra Dependencies Needed | Can be installed with | \n",
    "| :--: | -- | -- |\n",
    "| ExactMatch, F1 Score | No other dependencies are needed | -- |\n",
    "| bertscore | bert_score, evaluate | pip install azureml-metrics[bert-score] |\n",
    "\n",
    "To install all question-answering dependencies at once please run:\n",
    "\n",
    "```$ pip install azureml-metrics[qa]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT related metrics need openai_params to be computed. Computing metrics for ['f1_score', 'exact_match']\n",
      "LLM related metrics need llm_params to be computed. Computing metrics for ['f1_score', 'exact_match']\n",
      "GPT related metrics need openai_params in a dictionary.\n",
      "GPT related metrics need openai_params in a dictionary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'exact_match': [False, True, False, False],\n",
      "               'f1_score': [0.8000000000000002, 1.0, 0.5, 1.0]},\n",
      " 'metrics': {'mean_exact_match': 0.25,\n",
      "             'mean_f1_score': 0.8250000000000001,\n",
      "             'median_exact_match': 0.0,\n",
      "             'median_f1_score': 0.9000000000000001}}\n"
     ]
    }
   ],
   "source": [
    "# computing exact_match, f1 score for question answering task\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi 123\",\"foo bar foobar\", \"ram 234\", \"sid\"]\n",
    "y_test = [\"hello there general kenobi san\", \"foo bar foobar\", \"ram 23\", \"sid$\"]\n",
    "\n",
    "metrics_config = {\n",
    "    \"metrics\": [\"exact_match\", \"f1_score\"],\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, y_test=y_test, y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing bert-score please run:\n",
    "\n",
    "```$ pip install azureml-metrics[bert-score]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT related metrics need openai_params to be computed. Computing metrics for ['bertscore']\n",
      "LLM related metrics need llm_params to be computed. Computing metrics for ['bertscore']\n",
      "GPT related metrics need openai_params in a dictionary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'bertscore': {'f1': [0.693062424659729,\n",
      "                                    1.0,\n",
      "                                    0.7792337536811829,\n",
      "                                    0.435146301984787],\n",
      "                             'hashcode': 'microsoft/deberta-large_L16_no-idf_version=0.3.12(hug_trans=4.31.0)-rescaled',\n",
      "                             'precision': [0.6916525363922119,\n",
      "                                           1.0,\n",
      "                                           0.7782196998596191,\n",
      "                                           0.5183156728744507],\n",
      "                             'recall': [0.6915604472160339,\n",
      "                                        1.0,\n",
      "                                        0.7781534790992737,\n",
      "                                        0.3549974262714386]}},\n",
      " 'metrics': {'mean_bertscore_f1': 0.7268606200814247,\n",
      "             'mean_bertscore_precision': 0.7470469772815704,\n",
      "             'mean_bertscore_recall': 0.7061778381466866}}\n"
     ]
    }
   ],
   "source": [
    "# computing exact_match, f1 score for question answering task\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_pred = [\"hello there general kenobi 123\",\"foo bar foobar\", \"ram 234\", \"sid\"]\n",
    "y_test = [\"hello there general kenobi san\", \"foo bar foobar\", \"ram 23\", \"sid$\"]\n",
    "\n",
    "metrics_config = {\n",
    "    \"metrics\": [\"bertscore\"],\n",
    "}\n",
    "\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, y_test=y_test, y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing ada-cosine similarity, we need to have 'openai', 'plotly', 'scipy' dependencies installed. So, we can run:\n",
    "\n",
    "```$ pip install azureml-metrics[ada-cosine-similarity]```\n",
    "\n",
    "**Note: To compute ada-cosine-similarity we need to send openai credentials which has \"text-embedding-ada-002\" model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'ada_cosine_similarity': [0.9526062492711195,\n",
      "                                         1.0000000000000002,\n",
      "                                         0.9100055776481804,\n",
      "                                         0.8961406378903477]},\n",
      " 'metrics': {'mean_ada_cosine_similarity': 0.9396881162024119}}\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "y_pred = [\"hello there general kenobi 123\",\"foo bar foobar\", \"ram 234\", \"sid\"]\n",
    "y_test = [\"hello there general kenobi san\", \"foo bar foobar\", \"ram 23\", \"sid$\"]\n",
    "\n",
    "# this dictionary is propogated to openai completion or chat completion API.\n",
    "# please add the keys directly accepted by openai API.\n",
    "openai_params = {\n",
    "    \"api_version\": os.environ[\"OPENAI_API_VERSION\"],\n",
    "    \"api_base\": os.envrion[\"OPENAI_API_BASE\"],\n",
    "    \"api_type\": os.envrion[\"OPENAI_API_TYPE\"],\n",
    "    \"api_key\" : os.envrion[\"OPENAI_API_KEY\"],\n",
    "    \"deployment_id\": \"<deployment_id>\"\n",
    "}\n",
    "\n",
    "metrics_config = {\n",
    "     \"openai_params\" : openai_params,\n",
    "     # To compute gpt-star metrics\n",
    "     \"metrics\" : [\"ada_cosine_similarity\"]\n",
    "}\n",
    "\n",
    "# Note : length of lists of y_test, y_pred, questions, contexts should be equal\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, \n",
    "                         y_test=y_test,\n",
    "                         y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute all supported QA metrics:\n",
    "\n",
    "Now, as all required QA dependencies are installed: we can compute all required qa metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM related metrics need llm_params to be computed. Computing metrics for ['ada_similarity', 'bertscore', 'gpt_groundedness', 'gpt_relevance', 'f1_score', 'gpt_similarity', 'gpt_fluency', 'gpt_coherence', 'exact_match']\n",
      "Using the engine text-embedding-ada-002 for computing ada similarity. Please ensure to have valid deployment for text-embedding-ada-002 model\n",
      "Could not compute metric because of the following exception : Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'ada_similarity': ['notfounderror', 'notfounderror'],\n",
      "               'bertscore': {'f1': [1.0, 0.29999807476997375],\n",
      "                             'hashcode': 'microsoft/deberta-large_L16_no-idf_version=0.3.12(hug_trans=4.31.0)-rescaled',\n",
      "                             'precision': [1.0, 0.31754738092422485],\n",
      "                             'recall': [1.0, 0.27638280391693115]},\n",
      "               'exact_match': [True, False],\n",
      "               'f1_score': [1.0, 0.25],\n",
      "               'gpt_coherence': [5, 3],\n",
      "               'gpt_fluency': [5, 3],\n",
      "               'gpt_groundedness': [5, 3],\n",
      "               'gpt_relevance': [5, 1],\n",
      "               'gpt_similarity': [5, 1]},\n",
      " 'metrics': {'mean_ada_similarity': nan,\n",
      "             'mean_bertscore_f1': 0.6499990373849869,\n",
      "             'mean_bertscore_precision': 0.6587736904621124,\n",
      "             'mean_bertscore_recall': 0.6381914019584656,\n",
      "             'mean_exact_match': 0.5,\n",
      "             'mean_f1_score': 0.625,\n",
      "             'mean_gpt_coherence': 4.0,\n",
      "             'mean_gpt_fluency': 4.0,\n",
      "             'mean_gpt_groundedness': 4.0,\n",
      "             'mean_gpt_relevance': 3.0,\n",
      "             'mean_gpt_similarity': 3.0,\n",
      "             'median_ada_similarity': nan,\n",
      "             'median_exact_match': 0.5,\n",
      "             'median_f1_score': 0.625,\n",
      "             'median_gpt_coherence': 4.0,\n",
      "             'median_gpt_fluency': 4.0,\n",
      "             'median_gpt_groundedness': 4.0,\n",
      "             'median_gpt_relevance': 3.0,\n",
      "             'median_gpt_similarity': 3.0}}\n"
     ]
    }
   ],
   "source": [
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "context = \"In 2018, a group of scientists discovered a new type of deep-sea fish that has a transparent head. The fish, named Barreleye, has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "question = \"What is the name of the deep-sea fish discovered by scientists in 2018, and what is unique about its head?\"\n",
    "coherent_answer = \"The deep-sea fish discovered by scientists in 2018 is called Barreleye, and it has a transparent head. The fish has tubular eyes that can rotate to look either upward or forward, allowing it to see potential prey and predators in the dark depths of the ocean.\"\n",
    "incoherent_answer = \"The scientists who made the discovery in 2018 were actually studying coral reefs, not deep-sea fish. However, they did come across an unusual creature that they couldn't identify. It turned out to be a type of sea cucumber that has a strange, tube-like shape.\"\n",
    "\n",
    "# this dictionary is propogated to openai completion or chat completion API.\n",
    "# please add the keys directly accepted by openai API.\n",
    "openai_params = {\n",
    "    \"api_version\": os.environ[\"OPENAI_API_VERSION\"],\n",
    "    \"api_base\": os.envrion[\"OPENAI_API_BASE\"],\n",
    "    \"api_type\": os.envrion[\"OPENAI_API_TYPE\"],\n",
    "    \"api_key\" : os.envrion[\"OPENAI_API_KEY\"],\n",
    "    \"deployment_id\": \"<deployment_id>\"\n",
    "}\n",
    "\n",
    "\n",
    "metrics_config = {\n",
    "     \"questions\" : [question, question],\n",
    "     \"contexts\" : [context, context],\n",
    "     \"openai_params\" : openai_params,\n",
    "}\n",
    "\n",
    "# Note : length of lists of y_test, y_pred, questions, contexts should be equal\n",
    "result = compute_metrics(task_type=constants.Tasks.QUESTION_ANSWERING, \n",
    "                         y_test=[coherent_answer, coherent_answer],\n",
    "                         y_pred=[coherent_answer, incoherent_answer],\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For computing all of chat completion metrics, please run:\n",
    "\n",
    "```$ pip install azureml-metrics[chat-completion]```\n",
    "\n",
    "- The above command installs the following dependencies: (\"evaluate\", \"rouge-score\", \"torch\", \"transformers\", \"nltk\", \"rtoml\", \"azure-keyvault\", \"azure-identity\", \"requests\", \"aiohttp\", \"openai\", \"tenacity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Computing gpt groundedness score: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "Computing gpt relevance score: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n",
      "Computing gpt retrieval score: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'conversation_groundedness_score': [],\n",
      "               'gpt_groundedness': {'reason': [['<Quality reasoning:> The '\n",
      "                                                'chatbot\\'s response \"What is '\n",
      "                                                'the value of 2 + 2?\" is a '\n",
      "                                                'direct paraphrase of the '\n",
      "                                                'question \"2 + 2 = 4\". The '\n",
      "                                                'factual information in the '\n",
      "                                                \"chatbot's response, which is \"\n",
      "                                                'the equation \"2 + 2 = 4\", is '\n",
      "                                                'directly taken from the '\n",
      "                                                'retrieved document '\n",
      "                                                '\"math_document1.md\" where it '\n",
      "                                                'states \"2 + 2 = 4\". '\n",
      "                                                \"Therefore, the chatbot's \"\n",
      "                                                'response is grounded in the '\n",
      "                                                'retrieved documents.\\n'\n",
      "                                                '<Quality score: 5/5>\\n'\n",
      "                                                '<Input for Labeling End>'],\n",
      "                                               ['<Quality reasoning:> The '\n",
      "                                                \"chatbot's response is \"\n",
      "                                                'directly taken from the '\n",
      "                                                'retrieved document, which '\n",
      "                                                'states that Taj Mahal is '\n",
      "                                                'located in Agra, India.\\n'\n",
      "                                                '<Quality score: 5/5>\\n'\n",
      "                                                '<Input for Labeling End>']],\n",
      "                                    'score_per_conversation': [5.0, 5.0],\n",
      "                                    'score_per_turn': [[5], [5]]},\n",
      "               'gpt_relevance': {'reason': [['Quality score reasoning: The '\n",
      "                                             'provided response is a direct '\n",
      "                                             'and accurate answer to the '\n",
      "                                             'question. It correctly states '\n",
      "                                             'that the value of 2 + 2 is 4, '\n",
      "                                             'which is based on the addition '\n",
      "                                             'information retrieved from the '\n",
      "                                             'document.'],\n",
      "                                            ['Quality score reasoning: The '\n",
      "                                             'provided response is a direct '\n",
      "                                             'and accurate answer to the '\n",
      "                                             'question. It includes all the '\n",
      "                                             'necessary information based on '\n",
      "                                             'the conversation history and the '\n",
      "                                             'retrieved document.']],\n",
      "                                 'score_per_conversation': [5.0, 5.0],\n",
      "                                 'score_per_turn': [[5], [5]]},\n",
      "               'gpt_retrieval_score': {'reason': [['# Summarized Documents\\n'\n",
      "                                                   '- Document 1: Information '\n",
      "                                                   'about additions: 1 + 2 = '\n",
      "                                                   '3, 2 + 2 = 4\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Intent of the Question\\n'\n",
      "                                                   'The question is asking for '\n",
      "                                                   'the value of 2 + 2.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Document Relevance '\n",
      "                                                   'Scores\\n'\n",
      "                                                   '- Document 1 '\n",
      "                                                   '(math_document1.md): 5 '\n",
      "                                                   '(contains the exact value '\n",
      "                                                   'of 2 + 2)\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Overall Relevance\\n'\n",
      "                                                   'The document with ID '\n",
      "                                                   '\"math_document1.md\" is '\n",
      "                                                   'highly relevant to the '\n",
      "                                                   'given question as it '\n",
      "                                                   'directly provides the '\n",
      "                                                   'value of 2 + 2.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Result\\n'\n",
      "                                                   '5'],\n",
      "                                                  ['# Summarized Documents\\n'\n",
      "                                                   '- Document 1: Taj Mahal is '\n",
      "                                                   'located in Agra, India and '\n",
      "                                                   'is one of the seven '\n",
      "                                                   'wonders of the world.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Intent of the Question\\n'\n",
      "                                                   'The question is asking for '\n",
      "                                                   'the location of the Taj '\n",
      "                                                   'Mahal.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Document Relevance '\n",
      "                                                   'Scores\\n'\n",
      "                                                   '- Document 1 '\n",
      "                                                   '(taj_mahal_document1.md): '\n",
      "                                                   '5\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Overall Relevance\\n'\n",
      "                                                   'The given document '\n",
      "                                                   '(Document 1) provides the '\n",
      "                                                   'exact location of the Taj '\n",
      "                                                   'Mahal, making it highly '\n",
      "                                                   'relevant to the question.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '# Result\\n'\n",
      "                                                   '5']],\n",
      "                                       'score_per_conversation': [5.0, 5.0],\n",
      "                                       'score_per_turn': [[5], [5]]},\n",
      "               'perplexity': [45.78184509277344, 26.458881378173828]},\n",
      " 'metrics': {'bleu_1': 0.6153846153846154,\n",
      "             'bleu_2': 0.5793654595023211,\n",
      "             'bleu_3': 0.5303642318722039,\n",
      "             'bleu_4': 0.45437419561084635,\n",
      "             'mean_gpt_groundedness': 5.0,\n",
      "             'mean_gpt_relevance': 5.0,\n",
      "             'mean_gpt_retrieval_score': 5.0,\n",
      "             'mean_perplexity': 36.12036323547363,\n",
      "             'median_perplexity': 36.12036323547363,\n",
      "             'rouge1': 0.7222222222222222,\n",
      "             'rouge2': 0.6428571428571428,\n",
      "             'rougeL': 0.7222222222222222,\n",
      "             'rougeLsum': 0.7222222222222222}}\n",
      "CPU times: total: 5.48 s\n",
      "Wall time: 25.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpt4-32k model\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from pprint import pprint\n",
    "\n",
    "y_test = [[\"4\", \"2 + 2 = 4\"], [\"Agra\", \"Agra, India\"]]\n",
    "\n",
    "y_pred = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"2 + 2 = 4\",\n",
    "         \"context\": {\n",
    "             \"citations\": [{'id': 'math_document1.md',\n",
    "                            'content': 'Information about additions: ' \\\n",
    "                            '1 + 2 = 3, 2 + 2 = 4'}]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Where is Taj Mahal located?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Taj Mahal is located in Agra, India\",\n",
    "         \"context\": {\n",
    "             \"citations\": [{'id': 'taj_mahal_document1.md',\n",
    "                            'content': 'Taj Mahal is located in Agra, India ' \\\n",
    "                                        'and is one of the seven wonders of the world.'}]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "openai_params = {\n",
    "    \"api_version\": os.environ[\"OPENAI_API_VERSION\"],\n",
    "    \"api_base\": os.envrion[\"OPENAI_API_BASE\"],\n",
    "    \"api_type\": os.envrion[\"OPENAI_API_TYPE\"],\n",
    "    \"api_key\" : os.envrion[\"OPENAI_API_KEY\"],\n",
    "    \"deployment_id\": \"<deployment_id>\"\n",
    "}\n",
    "\n",
    "\n",
    "metrics_config = {\n",
    "    \"openai_params\": openai_params,\n",
    "    \"score_version\": \"v1\",\n",
    "    \"use_chat_completion_api\": True,\n",
    "    # To compute GPT based RAG metrics\n",
    "    # \"metrics\": [\"generation_score\", \"grounding_score\", \"retrieval_score\"]\n",
    "}\n",
    "\n",
    "# The above metrics can even be computed by setting the task_type to CHAT_COMPLETION\n",
    "result = compute_metrics(task_type=constants.Tasks.CHAT_COMPLETION,\n",
    "                         y_test=y_test, \n",
    "                         y_pred=y_pred,\n",
    "                         **metrics_config)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
